{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce995e24-c6de-4749-8658-fe88d8a6ede0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- using Service Principal connect to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4042d5de-6550-4695-b1d3-9fc2063a5e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Modified_by\", \"adfls_dev_cld_admin@am.elcompanies.net\")\n",
    "modified_by = dbutils.widgets.get(\"Modified_by\")\n",
    "modified_by = f\"'{modified_by}'\"\n",
    "print(modified_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "223c512e-487e-4a01-a2fe-c7bccd22f778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../RNDUtils/nb_utility\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc6982ca-e901-4e88-89d3-dc24c0ee18eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00513617-e26a-4b02-835d-bb67c3a0127b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature, AnalyzeResult,ContentFormat\n",
    "\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import logging, json, os\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74be00ce-a2e4-43e4-b74d-04ec763b0334",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Create AFR create_document_intelligence Client Object & analyse the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dbdbe6d-e666-4c13-a0c4-165f2acf6d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_document_intelligence_client(azure_form_rec_key, azure_form_rec_endpoint, api_version):\n",
    "    '''\n",
    "    Creates and returns an instance of the Document Intelligence Client for interacting with the Azure Form Recognizer service.\n",
    "\n",
    "    This function initializes the `DocumentIntelligenceClient` using the provided credentials and endpoint. It handles potential exceptions during the client creation process.\n",
    "\n",
    "    Parameters:\n",
    "        azure_form_rec_key (str): The API key for the Azure Form Recognizer service.\n",
    "        azure_form_rec_endpoint (str): The endpoint URL for the Azure Form Recognizer service.\n",
    "        api_version (str): The API version to be used for the service requests.\n",
    "\n",
    "    Returns:\n",
    "        DocumentIntelligenceClient or None: An instance of `DocumentIntelligenceClient` if creation is successful; otherwise, `None` if an exception occurs.\n",
    "    '''\n",
    "    try:\n",
    "        credential = AzureKeyCredential(azure_form_rec_key)\n",
    "        document_intelligence_client = DocumentIntelligenceClient(\n",
    "            endpoint=azure_form_rec_endpoint,\n",
    "            credential=AzureKeyCredential(azure_form_rec_key),\n",
    "            api_version=api_version\n",
    "        )\n",
    "        return document_intelligence_client\n",
    "    except Exception as error:\n",
    "        print(f\"An exception occurred while creating Document Intelligence Client: {error}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_intelligence_document(document_intelligence_client, azure_file_path_url):\n",
    "    '''\n",
    "    Analyzes a document using the Document Intelligence Client to extract key-value pairs and returns the result.\n",
    "\n",
    "    This function submits a document for analysis using the \"prebuilt-layout\" model and waits for the analysis to complete. It supports PDF documents and extracts key-value pairs in Markdown format.\n",
    "\n",
    "    Parameters:\n",
    "        document_intelligence_client (DocumentIntelligenceClient): An instance of the `DocumentIntelligenceClient` to interact with Azure Form Recognizer.\n",
    "        azure_file_path_url (str): The URL of the document to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple where the first element is a boolean indicating success (always `True` here), and the second element is the result of the document analysis.\n",
    "    '''\n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", \n",
    "        analyze_request=azure_file_path_url,\n",
    "        content_type=\"application/pdf\",\n",
    "        features=[DocumentAnalysisFeature.KEY_VALUE_PAIRS],\n",
    "        output_content_format=ContentFormat.MARKDOWN\n",
    "    )\n",
    "    result = poller.result()\n",
    "    return True, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6307266-81d3-4ab2-803e-d2cb4b67e232",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Create AFR create_document_analysis_client Client Object & analyse the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35e74d64-8722-4fa4-ba49-75641208c68d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_document_analysis_client(azure_form_rec_key, azure_form_rec_endpoint, api_version):\n",
    "    '''\n",
    "    Creates and returns an instance of the Document Analysis Client for interacting with Azure's Document Analysis service.\n",
    "\n",
    "    This function initializes the `DocumentAnalysisClient` using the provided API key, endpoint, and API version. It handles exceptions that may occur during the client creation process.\n",
    "\n",
    "    Parameters:\n",
    "        azure_form_rec_key (str): The API key for the Azure Document Analysis service.\n",
    "        azure_form_rec_endpoint (str): The endpoint URL for the Azure Document Analysis service.\n",
    "        api_version (str): The API version to use for the service requests.\n",
    "\n",
    "    Returns:\n",
    "        DocumentAnalysisClient or None: An instance of `DocumentAnalysisClient` if creation is successful; otherwise, `None` if an exception occurs.\n",
    "    '''\n",
    "    try:\n",
    "        credential = AzureKeyCredential(azure_form_rec_key)\n",
    "        \n",
    "        document_analysis_client = DocumentAnalysisClient(\n",
    "            endpoint=azure_form_rec_endpoint,\n",
    "            credential=AzureKeyCredential(azure_form_rec_key),\n",
    "            api_version=api_version\n",
    "        )\n",
    "        return document_analysis_client\n",
    "    except Exception as error:\n",
    "        print(f\"An exception occurred while creating Document Analysis Client: {error}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_document(document_analysis_client, azure_file_path_url):\n",
    "    '''\n",
    "    Analyzes a document using the Document Analysis Client to extract information based on the \"prebuilt-layout\" model.\n",
    "\n",
    "    This function submits the document specified by the URL for analysis and waits for the results. The analysis is performed using the prebuilt layout model, which extracts layout and structure information from the document.\n",
    "\n",
    "    Parameters:\n",
    "        document_analysis_client (DocumentAnalysisClient): An instance of the `DocumentAnalysisClient` for interacting with the Azure Document Analysis service.\n",
    "        azure_file_path_url (str): The URL of the document to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple where the first element is a boolean indicating success (always `True` here), and the second element is the result of the document analysis.\n",
    "    '''\n",
    "    poller = document_analysis_client.begin_analyze_document(\n",
    "        \"prebuilt-layout\", \n",
    "        document=azure_file_path_url,\n",
    "    )\n",
    "    result = poller.result()\n",
    "    return True, result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88ef7096-24fe-4e64-bc94-65339fb88a95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Create ADLS client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26332811-219b-4c05-88d5-443bcd204a91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_adls_service_client(adls_account_name, adls_file_system, azure_file_system_key, adls_directory):\n",
    "    '''\n",
    "    Connects to Azure Data Lake Storage (ADLS) and returns a file system client.\n",
    "\n",
    "    This function establishes a connection to Azure Data Lake Storage using the provided account name and file system key, retrieves the file system client, and creates a directory if it does not already exist.\n",
    "\n",
    "    Parameters:\n",
    "        adls_account_name (str): The name of the Azure Data Lake Storage account.\n",
    "        adls_file_system (str): The name of the file system within the ADLS account.\n",
    "        azure_file_system_key (str): The key used for authenticating with the ADLS account.\n",
    "        adls_directory (str): The name of the directory to create within the file system.\n",
    "\n",
    "    Returns:\n",
    "        DataLakeFileSystemClient or None: An instance of `DataLakeFileSystemClient` if the connection is successful and the directory is created; otherwise, `None` if an exception occurs.\n",
    "    '''\n",
    "    try:\n",
    "        # Connect to Azure Data Lake Storage\n",
    "        service_client = DataLakeServiceClient(account_url=f\"https://{adls_account_name}.dfs.core.windows.net\",\n",
    "                                                credential=azure_file_system_key)\n",
    "        # Get file system client\n",
    "        adls_file_system_client = service_client.get_file_system_client(adls_file_system)\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        adls_file_system_client.create_directory(adls_directory)\n",
    "\n",
    "        return adls_file_system_client\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while connecting to Azure Data Lake Storage: {e}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "def save_to_adls(adls_file_system_client, file_name, json_str):\n",
    "    '''\n",
    "    Uploads a string of JSON data to Azure Data Lake Storage.\n",
    "\n",
    "    This function uploads the provided JSON string to a specified file within the ADLS file system. If the file already exists, it will be overwritten.\n",
    "\n",
    "    Parameters:\n",
    "        adls_file_system_client (DataLakeFileSystemClient): An instance of the `DataLakeFileSystemClient` used for interacting with the Azure Data Lake Storage.\n",
    "        file_name (str): The name of the file where the JSON data should be uploaded.\n",
    "        json_str (str): The JSON string to be uploaded to the file.\n",
    "\n",
    "    Returns:\n",
    "        bool: `True` if the upload is successful; otherwise, `False` if an exception occurs or the file system client is not provided.\n",
    "    '''\n",
    "    try:\n",
    "        if adls_file_system_client:\n",
    "            # Get file client\n",
    "            file_client = adls_file_system_client.get_file_client(file_name)\n",
    "            file_client.upload_data(json_str, overwrite=True)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while uploading file to ADLS: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8690726-eb7d-4cd1-84ac-3a242fe7ba13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Read Credentials from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf89c9e-6488-421a-9250-e2bd89c31b15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get credentials from config\n",
    "utility_obj = Utility()\n",
    "AFR_Config_dict = utility_obj.AFR_Config\n",
    "Naming_dict = utility_obj.naming_dict\n",
    "\n",
    "spark = SparkSession.builder.appName('delta tables').getOrCreate()\n",
    "\n",
    "AZURE_FORM_REC_KEY = AFR_Config_dict[\"AZURE_FORM_REC_KEY\"]\n",
    "AZURE_FORM_REC_ENDPOINT = AFR_Config_dict[\"AZURE_FORM_REC_ENDPOINT\"]\n",
    "ADLS_ACCOUNT_NAME = AFR_Config_dict[\"ADLS_ACCOUNT_NAME\"]\n",
    "ADLS_FILE_SYSTEM = AFR_Config_dict[\"ADLS_FILE_SYSTEM\"]\n",
    "ADLS_DIRECTORY = AFR_Config_dict[\"ADLS_DIRECTORY\"]\n",
    "JSON_RESULT_DIRECTORY = AFR_Config_dict[\"JSON_RESULT_DIRECTORY\"]\n",
    "ADLS_FILE_SYSTEM_KEY = AFR_Config_dict[\"ADLS_FILE_SYSTEM_KEY\"]\n",
    "api_version_intelligence = AFR_Config_dict[\"AFR_API_VERSION_Intelligence_document\"]\n",
    "api_version_general = AFR_Config_dict[\"AFR_API_VERSION_analyse_document\"]\n",
    "ingest_table_name = Naming_dict[\"ingest_audit_dimension\"][\"table_name\"]\n",
    "proc_table_name = Naming_dict[\"proc_audit_dimension\"][\"table_name\"]\n",
    "ingest_table_seq_no = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"IA_SEQ_NO\"]\n",
    "ingest_table_version = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"IA_VERSION\"]\n",
    "ingest_table_dataset_id = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"DATASET_ID\"]\n",
    "ingest_table_report_test_type = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"REPORT_TEST_TYPE\"]\n",
    "ingest_table_study_id = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"STUDY_ID\"]\n",
    "ingest_table_adls_proc_container_path = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"ADLS_PROC_CONTAINER_PATH\"]\n",
    "ingest_table_filename = Naming_dict[\"ingest_audit_dimension\"][\"columns\"][\"FILE_NAME\"]\n",
    "proc_table_seq_no = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"PA_SEQ_NO\"]\n",
    "proc_table_version = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"PA_VERSION\"]\n",
    "proc_table_dataset_id  = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"DATASET_ID\"]\n",
    "proc_table_afr_status = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"AFR_STATUS\"]\n",
    "proc_table_afr_raw_json_path = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"AFR_RAW_JSON_PATH\"]\n",
    "proc_table_afr_datetime = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"AFR_DATETIME\"]\n",
    "proc_table_pa_modified_by = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"PA_MODIFIED_BY\"]\n",
    "proc_table_pa_modified_date = Naming_dict[\"proc_audit_dimension\"][\"columns\"][\"PA_MODIFIED_DATE\"]\n",
    "print(api_version_general, api_version_intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296bb263-1df5-4cf2-9c96-e7f8a5d875c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_ingest_audit_dim_df():\n",
    "    '''\n",
    "    Reads and filters data from an ingestion audit table.\n",
    "\n",
    "    This function executes a SQL query on a Spark DataFrame to retrieve records from an ingestion audit table where:\n",
    "    - The file name ends with `.pdf` or `.PDF`.\n",
    "    - The report test type is either 'TAC' or 'Consumer Science'.\n",
    "    - The status code is 'PROC_COMPLETE'.\n",
    "\n",
    "    The query selects all columns from the table that match these criteria.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.dataframe.DataFrame: A Spark DataFrame containing the filtered records from the ingestion audit table.\n",
    "    '''\n",
    "    spark_ingest_df = spark.sql(f\"\"\"\n",
    "        SELECT * \n",
    "        FROM {ingest_table_name}\n",
    "        WHERE \n",
    "            (FILE_NAME LIKE '%.pdf' OR FILE_NAME LIKE '%.PDF') \n",
    "            AND \n",
    "            (\n",
    "                REPORT_TEST_TYPE = 'TAC' \n",
    "                OR\n",
    "                REPORT_TEST_TYPE = 'Consumer Science' \n",
    "            ) \n",
    "            AND\n",
    "            STATUS_CODE = 'PROC_COMPLETE'\n",
    "    \"\"\")\n",
    "    return spark_ingest_df\n",
    "\n",
    "# Example usage\n",
    "# df = read_ingest_audit_dim_df()\n",
    "# df.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c235e9-dab1-462c-bc9f-3886ecd9bb4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_proc_success_audit_table(seq_no_column1, seq_value1, updates):\n",
    "    '''\n",
    "    Updates specified columns in the process success audit table.\n",
    "\n",
    "    This function constructs and executes an SQL `UPDATE` statement to modify records in the process success audit table.\n",
    "    The columns to be updated and their new values are specified by the `updates` dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        seq_no_column1 (str): The name of the column used to identify the record to be updated.\n",
    "        seq_value1 (str or int): The value in the `seq_no_column1` column that identifies which record to update.\n",
    "        updates (dict): A dictionary where the keys are column names and the values are the new values to set for those columns.\n",
    "        \n",
    "    Returns:\n",
    "        None: This function does not return a value.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs while executing the SQL query.\n",
    "    '''\n",
    "    try:\n",
    "        # Construct the SET part of the SQL query\n",
    "        set_clause = \", \".join([f\"{column} = {value}\" for column, value in updates.items()])\n",
    "        \n",
    "        # Construct and execute the SQL query\n",
    "        spark.sql(f\"\"\"UPDATE {proc_table_name} SET \n",
    "                      {set_clause}\n",
    "                      WHERE {seq_no_column1} = {seq_value1}\n",
    "                      \"\"\")\n",
    "        print(f\"Successfully updated the columns with values {updates}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while updating the audit table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaa53f58-1025-4ede-bb2d-2c03a8fc675e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_proc_audit_table_afr_status(seq_no):\n",
    "    '''\n",
    "    Checks the AFR status of a record in the process audit table.\n",
    "\n",
    "    This function queries the process audit table to determine the AFR (Application For Review) status of a record\n",
    "    identified by the given sequence number (`seq_no`). It then prints the status and returns a boolean indicating\n",
    "    whether the AFR status is \"Success\".\n",
    "\n",
    "    Parameters:\n",
    "        seq_no (int or str): The sequence number of the record to check in the process audit table.\n",
    "\n",
    "    Returns:\n",
    "        bool: \n",
    "            - `True` if the AFR status of the record is 'Success'.\n",
    "            - `False` if the AFR status is not 'Success' or if the record does not exist.\n",
    "\n",
    "    Notes:\n",
    "        - This function assumes that `proc_table_name` and `proc_table_afr_status` are predefined variables in the\n",
    "          environment where the function is executed.\n",
    "        - The function prints a message to the console indicating whether the AFR status is 'Success' or 'New'.\n",
    "    '''\n",
    "    # Read the table with a filter applied\n",
    "    df = spark.table(proc_table_name).filter(f\"PA_SEQ_NO = {seq_no}\")\n",
    "    # Collect the filtered results\n",
    "    existing_record = df.select(proc_table_afr_status).collect()\n",
    "    # Check if there is any record and process it\n",
    "    if existing_record:\n",
    "        afr_status = existing_record[0][proc_table_afr_status]\n",
    "        if afr_status == 'Success':\n",
    "            print(f\"For seq no {seq_no}, AFR status is Already Success!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"For seq no {seq_no}, AFR status is New!\")\n",
    "            return False\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d653ec6-8802-45bb-ba7d-8724abc2d62c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_proc_audit_table(new_row):\n",
    "    '''\n",
    "    Inserts a new record into the process audit table if a record with the same sequence number does not already exist.\n",
    "\n",
    "    This function attempts to insert a new row into the process audit table. It first checks if a record with the\n",
    "    same sequence number (`PA_SEQ_NO`) already exists in the table. If no such record is found, it inserts the new\n",
    "    row. If a record with the same sequence number already exists, it skips the insertion.\n",
    "\n",
    "    Parameters:\n",
    "        new_row (list or DataFrame): A list of tuples or a DataFrame containing the new record to be inserted. \n",
    "                                      The record should match the schema of the existing process audit table.\n",
    "\n",
    "    Returns:\n",
    "        bool: `True` if the new record is inserted or if a record with the same sequence number already exists.\n",
    "              Returns `False` only if there was an issue inserting the record (not handled in this implementation).\n",
    "\n",
    "    Notes:\n",
    "        - `proc_table_name` should be defined in the environment where this function is executed.\n",
    "        - The function prints a message indicating whether the record was successfully inserted or if a duplicate was found.\n",
    "        - If using a DataFrame, ensure `new_row` has the correct schema as expected by the process audit table.\n",
    "    '''\n",
    "    df = spark.table(proc_table_name)\n",
    "    new_df = spark.createDataFrame(new_row, schema=df.schema)\n",
    "    \n",
    "    # Extract seq_no from the new row\n",
    "    seq_no = new_row[0][0]\n",
    "    \n",
    "    # Check if a record with the same seq_no already exists\n",
    "    existing_record = df.filter(df.PA_SEQ_NO == seq_no).collect()\n",
    "    \n",
    "    if not existing_record:\n",
    "        # If no such record exists, insert the new row\n",
    "        new_df.createOrReplaceTempView('new_row')\n",
    "        spark.sql(f\"\"\"INSERT INTO {proc_table_name} SELECT * FROM new_row\"\"\")\n",
    "        print(f\"Inserted new record for seq_no {seq_no}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Record with seq_no {seq_no} already exists. No new record inserted.\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7531f854-b3d7-446e-8b4e-3c4581933be7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_seq_no_version_and_study_type(ingest_df, path):\n",
    "    '''\n",
    "    Retrieves sequence number, file version number, dataset ID, and study type for a given file path from the ingest DataFrame.\n",
    "\n",
    "    This function filters the ingest DataFrame based on the provided file path and extracts the sequence number, \n",
    "    file version number, dataset ID, and study type associated with that path.\n",
    "\n",
    "    Parameters:\n",
    "        ingest_df (DataFrame): The DataFrame containing ingestion data with columns for sequence number, file version, \n",
    "                               dataset ID, and study type.\n",
    "        path (str): The file path used to filter the DataFrame. Only rows with a matching `full_process_layer_path` \n",
    "                    will be considered.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing four elements:\n",
    "            - `seq_no` (int): The sequence number associated with the file path, or `None` if no matching row is found.\n",
    "            - `file_version_no` (int): The file version number associated with the file path, or `None` if no matching row is found.\n",
    "            - `dataset_id` (int): The dataset ID associated with the file path, or `None` if no matching row is found.\n",
    "            - `study_type` (str): The study type associated with the file path, or `None` if no matching row is found.\n",
    "    '''\n",
    "    new_df = ingest_df[ingest_df[\"full_process_layer_path\"] == path]\n",
    "    if not new_df.empty:\n",
    "        seq_no = new_df[ingest_table_seq_no].iloc[0]\n",
    "        file_version_no = new_df[ingest_table_version].iloc[0]\n",
    "        dataset_id = new_df[ingest_table_dataset_id].iloc[0]\n",
    "        study_type = new_df[ingest_table_report_test_type].iloc[0]\n",
    "        return int(seq_no), int(file_version_no), int(dataset_id), study_type\n",
    "    else:\n",
    "        return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f3a40cb-1634-45a7-ab3d-5fe82e41455d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get documnet intelligence client object\n",
    "document_intelligence_client = create_document_intelligence_client(AZURE_FORM_REC_KEY, AZURE_FORM_REC_ENDPOINT, api_version_intelligence)\n",
    "\n",
    "# get documnet analysis client object\n",
    "document_analysis_client = create_document_analysis_client(AZURE_FORM_REC_KEY, AZURE_FORM_REC_ENDPOINT, api_version_general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22cf09c4-911e-4c08-a8c1-f2a59be54f12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_study_type(study_type, pdf_data):\n",
    "    '''\n",
    "    Analyzes a PDF document based on the study type and returns the analysis result as a JSON string.\n",
    "\n",
    "    Depending on the study type specified, this function utilizes different methods to analyze the PDF document \n",
    "    and returns the status of the analysis along with the result in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "        study_type (str): The type of study that determines the analysis method. Expected values are \"TAC\" or \n",
    "                          \"Consumer Science\".\n",
    "        pdf_data (str): The path or URL to the PDF document to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - `afr_status` (bool): A boolean indicating the success of the analysis.\n",
    "            - `json_string` (str or None): The analysis result in JSON format if successful, otherwise `None`.\n",
    "    '''\n",
    "    if study_type == \"TAC\":\n",
    "        afr_status, result_json = analyze_document(document_analysis_client, pdf_data)\n",
    "        json_string = json.dumps(result_json.to_dict(), indent=4)\n",
    "        return afr_status, json_string\n",
    "\n",
    "    elif study_type == \"Consumer Science\":\n",
    "        afr_status, result_json = analyze_document(document_analysis_client, pdf_data)\n",
    "        json_string = json.dumps(result_json.to_dict(), indent=4)\n",
    "        return afr_status, json_string\n",
    "    else:\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56492c9a-e815-4737-b10c-fcc185699142",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# get ADLS client object\n",
    "adls_file_system_client = get_adls_service_client(ADLS_ACCOUNT_NAME, \n",
    "                                                  ADLS_FILE_SYSTEM, \n",
    "                                                  ADLS_FILE_SYSTEM_KEY, \n",
    "                                                  ADLS_DIRECTORY)\n",
    "\n",
    "# read ingest table file  \n",
    "ingest_df = read_ingest_audit_dim_df().toPandas()\n",
    "\n",
    "if not ingest_df.empty:\n",
    "    ingest_df[\"full_process_layer_path\"] = ingest_df[ingest_table_adls_proc_container_path] + \"/\" + ingest_df[ingest_table_filename]\n",
    "    ingest_file_paths = ingest_df[\"full_process_layer_path\"].tolist()\n",
    "\n",
    "    print(\"Total Files to be ingest : \", len(ingest_file_paths))\n",
    "else:\n",
    "    print(\"Ingest Table is empty!\")\n",
    "\n",
    "# print(df.shape)                                           \n",
    "# get all list of files from direcory\n",
    "file_paths = adls_file_system_client.get_paths(path=ADLS_DIRECTORY)\n",
    "\n",
    "# iterate each list of files\n",
    "if len(ingest_file_paths) > 0:\n",
    "    for file_path in ingest_file_paths:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            print(\"##########################\")\n",
    "            print(file_path)\n",
    "\n",
    "            try:\n",
    "                # fetch SEQ NO from Audit \n",
    "                seq_no, file_version_no, dataset_id, study_type  = get_seq_no_version_and_study_type(ingest_df, file_path)\n",
    "                afr_success_status = check_proc_audit_table_afr_status(seq_no=seq_no)\n",
    "                if afr_success_status:\n",
    "                    continue\n",
    "                # run rest of the logic where AFR status is not Success\n",
    "                # Read each file from ADLS\n",
    "                file_client = adls_file_system_client.get_file_client(file_path)\n",
    "                download = file_client.download_file()\n",
    "                pdf_data = download.readall()\n",
    "                \n",
    "                # Pass to AFR & get result\n",
    "                afr_status, json_string = check_study_type(study_type, pdf_data)\n",
    "                print(afr_status, \"afr_status\")\n",
    "                \n",
    "                # Dump AFR result into JSON file\n",
    "                if afr_status and json_string:\n",
    "                    json_file_path = file_path.split(\"/\")[-1].replace(\".pdf\", \".pdf.json\").replace(\".PDF\", \".PDF.json\")\n",
    "                    \n",
    "                    json_file_path = str(dataset_id)+\"_\"+str(file_version_no)+\"_\"+str(json_file_path) \n",
    "                    \n",
    "                    json_file_path = os.path.join(JSON_RESULT_DIRECTORY, json_file_path)\n",
    "                    save_status = save_to_adls(adls_file_system_client, json_file_path, json_string) \n",
    "                    \n",
    "                    print(f\"JSON File Saved ---> {json_file_path}\\n\")\n",
    "                    \n",
    "                    # Update Audit table\n",
    "                    if seq_no and save_status:\n",
    "                        new_row = [[seq_no,file_version_no,dataset_id,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,modified_by,modified_by,datetime.now(),datetime.now()]]\n",
    "                        new_row_status = insert_proc_audit_table(new_row)\n",
    "                        if new_row_status:\n",
    "                            json_file_path = json_file_path.replace(\"'\", \"%27\")\n",
    "                            json_file_path = f\"\"\"'{json_file_path}'\"\"\"\n",
    "                            now = datetime.now()#.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                            formatted_datetime = f\"'{now}'\"\n",
    "                            updates = {\n",
    "                                proc_table_afr_status : \"'Success'\",\n",
    "                                proc_table_afr_raw_json_path : json_file_path,\n",
    "                                proc_table_afr_datetime: formatted_datetime,\n",
    "                                proc_table_pa_modified_by: modified_by,\n",
    "                                proc_table_pa_modified_date: formatted_datetime\n",
    "                            }\n",
    "                            update_proc_success_audit_table(proc_table_seq_no, seq_no, updates)\n",
    "                            \n",
    "                else:\n",
    "                    now = datetime.now()#.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                    formatted_datetime = f\"'{now}'\"\n",
    "                    updates = {\n",
    "                                proc_table_afr_status: \"'Failed'\",\n",
    "                                proc_table_afr_datetime: formatted_datetime,\n",
    "                                proc_table_pa_modified_by: modified_by,\n",
    "                                proc_table_pa_modified_date: formatted_datetime\n",
    "                            }\n",
    "                    update_proc_success_audit_table(proc_table_seq_no, seq_no,  updates)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while calling AFR parser and storing into JSON: {e}\")\n",
    "                if seq_no and file_version_no:\n",
    "                    now = datetime.now()#.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "                    formatted_datetime = f\"'{now}'\"\n",
    "                    updates = {\n",
    "                                proc_table_afr_status: \"'Failed'\",\n",
    "                                proc_table_afr_datetime: formatted_datetime,\n",
    "                                proc_table_pa_modified_by: modified_by,\n",
    "                                proc_table_pa_modified_date: formatted_datetime\n",
    "                    }\n",
    "                    update_proc_success_audit_table(proc_table_seq_no, seq_no, updates)\n",
    "                continue\n",
    "\n",
    "    print(\"AFR Parsing is Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfb15874-888e-49e8-9e97-a7f0eafa7def",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"AFR Parsing is Done!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "nb_AFR_PDF_Parsing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
