{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be4531aa-e115-4475-86b9-6e111b799316",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import itertools\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from types import SimpleNamespace\n",
    "from dataclasses import dataclass\n",
    "from collections import OrderedDict\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as ptypes\n",
    "from openai import OpenAI\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "120ccd3c-2dc4-4612-a454-5b1aa00bda45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# PDF preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a811ab8b-9a93-48a9-b260-bd585f814bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scale_y_coordinates(bounding_box, vertical_offset):\n",
    "    # get bounding box as tuplea\n",
    "    bbox = [(xy['x'], xy['y']) for xy in bounding_box]\n",
    "    (x1, y1), (x2, y2), (x3, y3), (x4, y4) = bbox\n",
    "    # scale the coordinates\n",
    "    abs_x1 = x1\n",
    "    abs_y1 = vertical_offset + y1\n",
    "    abs_x2 = x2\n",
    "    abs_y2 = vertical_offset + y2\n",
    "    abs_x3 = x3\n",
    "    abs_y3 = vertical_offset + y3\n",
    "    abs_x4 = x4\n",
    "    abs_y4 = vertical_offset + y4\n",
    "    # absoulte bounding box\n",
    "    abs_bbox = [{'x': abs_x1, 'y': abs_y1},\n",
    "                {'x': abs_x2, 'y': abs_y2},\n",
    "                {'x': abs_x3, 'y': abs_y3},\n",
    "                {'x': abs_x4, 'y': abs_y4}]\n",
    "    return abs_bbox\n",
    "\n",
    "\n",
    "def compute_paragraphs_absolute_coordinates(pdf):\n",
    "    cumulative_vertical_offset = 0\n",
    "    # Iterate over each page\n",
    "    for page in pdf[\"pages\"]:\n",
    "        page_height = page[\"height\"]\n",
    "        # Iterate over paragraphs\n",
    "        for i, para in enumerate(pdf[\"paragraphs\"]):\n",
    "            para_bbox = para[\"bounding_regions\"][0][\"polygon\"]\n",
    "            page_num = para[\"bounding_regions\"][0][\"page_number\"]\n",
    "            if page_num > page[\"page_number\"]:\n",
    "                break\n",
    "            if page_num == page[\"page_number\"]:\n",
    "                # absolute coordinates\n",
    "                abs_bbox = scale_y_coordinates(para_bbox, cumulative_vertical_offset)\n",
    "                # add absolute coordinates\n",
    "                para[\"bounding_regions\"][0][\"absolute_bbox\"] = abs_bbox\n",
    "                # add the table with absolue coordinates\n",
    "                pdf[\"paragraphs\"][i] = para\n",
    "        # update cumulative vertical offset for the next page\n",
    "        cumulative_vertical_offset += page_height\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def compute_tables_absolute_coordinates(pdf):\n",
    "    cumulative_vertical_offset = 0\n",
    "    # Iterate over each para\n",
    "    for page in pdf[\"pages\"]:\n",
    "        page_height = page[\"height\"]\n",
    "        for i, table in enumerate(pdf[\"tables\"]):\n",
    "            table_bbox = table[\"bounding_regions\"][0][\"polygon\"]\n",
    "            page_num = table[\"bounding_regions\"][0][\"page_number\"]\n",
    "            if page_num == page[\"page_number\"]:\n",
    "                # absolute coordinates\n",
    "                abs_bbox = scale_y_coordinates(table_bbox, cumulative_vertical_offset)\n",
    "                # add absolute coordinates\n",
    "                table[\"bounding_regions\"][0][\"absolute_bbox\"] = abs_bbox\n",
    "                # add absolute coordinates for each cell\n",
    "                for j, cell in enumerate(table[\"cells\"]):\n",
    "                    cell_bbox = cell[\"bounding_regions\"][0][\"polygon\"]\n",
    "                    cell_abs_bbox = scale_y_coordinates(cell_bbox, cumulative_vertical_offset)\n",
    "                    cell[\"bounding_regions\"][0][\"absolute_bbox\"] = cell_abs_bbox\n",
    "                    table[\"cells\"][j] = cell\n",
    "                # add the table with absolue coordinates\n",
    "                pdf[\"tables\"][i] = table\n",
    "            # update cumulative vertical offset for the next page\n",
    "        cumulative_vertical_offset += page_height\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ac20fe-87c7-4dba-9993-15b85fd1ae2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "re.search(  r'^(\\d{1,2})\\.(\\d{1,2})\\s+(?!No change)((?!%)[^0-9]*)$', '0.1 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c25c48-d38c-4e05-8a7a-6e07d46a295d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_toc_page(pdf):\n",
    "    toc_list = [\"table of contents\", \"chapters\", \"chapter list\", \"toc\"]\n",
    "    index_page = -1\n",
    "    flag_toc = False\n",
    "\n",
    "    for page in pdf[\"pages\"]:\n",
    "        num_sections_or_subsections = 0\n",
    "        lines = page[\"lines\"]\n",
    "\n",
    "        for i in range(len(lines)-1):\n",
    "            curr_line = lines[i][\"content\"].strip()\n",
    "            next_line = lines[i+1][\"content\"].strip()\n",
    "\n",
    "            if curr_line.lower() in toc_list:\n",
    "                flag_toc = True\n",
    "\n",
    "            if ( re.match(r'^(\\d+)\\.0', curr_line, re.IGNORECASE) and next_line.isupper() ) or \\\n",
    "                ( re.match(r'(\\d+)\\.(\\d+)', curr_line, re.IGNORECASE) and re.sub(r\"[^A-Za-z]\", \"\", next_line).isalpha() ) or \\\n",
    "                re.match(r'^(\\d+)\\.0\\s+(.*)$', curr_line, re.IGNORECASE) or  re.match(r'^(\\d+)\\.(\\d+)\\s+(.*)$', curr_line, re.IGNORECASE):\n",
    "                num_sections_or_subsections += 1\n",
    "\n",
    "        if num_sections_or_subsections >= 10:\n",
    "            index_page = page[\"page_number\"]\n",
    "            break\n",
    "    #print(flag_toc, num_sections_or_subsections)\n",
    "    if flag_toc and index_page != -1:\n",
    "        return index_page\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "def extract_sectionwise_text(pdf):\n",
    "    exh_main_list = [\"TITLE\", \"OBJECTIVES\", \"TEST PRODUCTS\", \"METHODOLOGY\", \"INFORMED CONSENT\", \"STUDY PANEL\", \"PANELIST WITHDRAWAL\",\n",
    "                     \"PANELIST DISPOSITION AND DEMOGRAPHICS\", \"PANELIST ACCOUNTABILITY\", \"ADVERSE EVENTS\", \"RESULTS\", \"DEVIATIONS\",\n",
    "                     \"CONCLUSION\", \"REPORT APPROVAL\", \"APPENDIX\"]\n",
    "    exh_sub_list = ['Product Description', 'Product Use Instructions', 'Compliance', 'Inclusion Criteria', 'Exclusion Criteria',\n",
    "                    'Panelist Accountability', 'Definition of an Adverse Event', 'Definition of a \"Serious\" Adverse Event',\n",
    "                    'Documentation of Adverse Events', 'Evaluations', 'Skin Evaluation by Study Personnel', 'Adverse Events and/or Discomfort mentions',\n",
    "                    'Additional Information for Adverse Events and Discomfort Mentions', 'Investigator Conclusion', \"Dermatologist's Conclusion\", \"Opthalmologist's Conclusion\"]\n",
    "\n",
    "    main_sections = []\n",
    "    curr_main_section = None\n",
    "    curr_subsection = None\n",
    "    section_number = '0'\n",
    "\n",
    "    # Regex pattern to match main section headings like \"x.0 Section Name\"\n",
    "    main_section_pattern = r'^(\\d{1,2})\\.0\\s+([^0-9]*)$'\n",
    "    # Regex pattern to match subsection headings like \"x.y Subsection Name\"\n",
    "    subsection_pattern = r'^(\\d{1,2})\\.(\\d{1,2})\\s+((?!%)[^0-9]*)$'\n",
    "    # Iterate through each paragraph in the layout result\n",
    "    toc_page_num = get_toc_page(pdf)\n",
    "    print('#######################################')\n",
    "    print('toc_page_number: ',toc_page_num)\n",
    "    paragraphs = list(filter(lambda para: para[\"bounding_regions\"][0][\"page_number\"] > toc_page_num, pdf[\"paragraphs\"]))\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        text = paragraph[\"content\"].strip()\n",
    "        abs_bbox = paragraph[\"bounding_regions\"][0][\"absolute_bbox\"]\n",
    "\n",
    "        if re.match(r\"^(Docusign Envelope).*|^(RRPORT-IUS).*\", text, re.IGNORECASE): continue\n",
    "\n",
    "        # Check if the paragraph matches the main section pattern\n",
    "        main_match = re.match(main_section_pattern, text)\n",
    "        if main_match:\n",
    "            # If there was a previous main section, append it to main_sections\n",
    "            if curr_main_section:\n",
    "                if curr_subsection:\n",
    "                    curr_main_section[\"subsections\"].append(curr_subsection)\n",
    "                    curr_subsection = None\n",
    "                main_sections.append(curr_main_section)\n",
    "            # Initialize a new main section\n",
    "            section_number = main_match.group(1)\n",
    "            section_name = main_match.group(2)\n",
    "            curr_main_section = {\n",
    "                \"number\": section_number,\n",
    "                \"name\": section_name,\n",
    "                \"absolute_bbox\": abs_bbox,\n",
    "                \"tables\": [],\n",
    "                \"content\": [],\n",
    "                \"subsections\": [],\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        elif text in exh_main_list:\n",
    "            # If there was a previous main section, append it to main_sections\n",
    "            if curr_main_section:\n",
    "                if curr_subsection:\n",
    "                    curr_main_section[\"subsections\"].append(curr_subsection)\n",
    "                    curr_subsection = None\n",
    "                main_sections.append(curr_main_section)\n",
    "            # Initialize a new main section\n",
    "            section_number = str(float(section_number) + 1)\n",
    "            section_name = text\n",
    "            curr_main_section = {\n",
    "                \"number\": section_number,\n",
    "                \"name\": section_name,\n",
    "                \"absolute_bbox\": abs_bbox,\n",
    "                \"tables\": [],\n",
    "                \"content\": [],\n",
    "                \"subsections\": [],\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # Check if the paragraph matches the subsection pattern\n",
    "        subsection_match = re.match(subsection_pattern, text)\n",
    "        if subsection_match and curr_main_section:\n",
    "            # If there was a previous subsection, append it to curr_main_section's subsections\n",
    "            if curr_subsection:\n",
    "                curr_main_section[\"subsections\"].append(curr_subsection)\n",
    "            # Initialize a new subsection\n",
    "            section_number = subsection_match.group(1)\n",
    "            subsection_number = subsection_match.group(2)\n",
    "            subsection_name = subsection_match.group(3)\n",
    "            curr_subsection = {\n",
    "                \"main_section_number\": section_number,\n",
    "                \"subsection_number\": subsection_number,\n",
    "                \"name\": subsection_name,\n",
    "                \"absolute_bbox\": abs_bbox,\n",
    "                \"tables\": [],\n",
    "                \"content\": [],\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        # elif text in exh_sub_list:\n",
    "        #     section_number = curr_main_section[\"number\"]\n",
    "        #     subsection_number = None\n",
    "        #     subsection_name = text\n",
    "        #     curr_subsection = {\n",
    "        #         \"main_section_number\": section_number,\n",
    "        #         \"subsection_number\": subsection_number,\n",
    "        #         \"name\": subsection_name,\n",
    "        #         \"absolute_bbox\": abs_bbox,\n",
    "        #         \"tables\": [],\n",
    "        #         \"content\": [],\n",
    "        #     }\n",
    "        #     continue\n",
    "\n",
    "        # If the paragraph doesn't match any pattern, add it to curr_subsection's content if exists\n",
    "        if curr_subsection:\n",
    "            curr_subsection[\"content\"].append({\"text\": text, \"absolute_bbox\": abs_bbox})\n",
    "        elif curr_main_section:\n",
    "            curr_main_section[\"content\"].append({\"text\": text, \"absolute_bbox\": abs_bbox})\n",
    "    # Append the last main section and its last subsection if they exist\n",
    "    if curr_main_section:\n",
    "        if curr_subsection:\n",
    "            curr_main_section[\"subsections\"].append(curr_subsection)\n",
    "        main_sections.append(curr_main_section)\n",
    "\n",
    "    return main_sections\n",
    "\n",
    "\n",
    "# sections = extract_sectionwise_text(pdf)\n",
    "# sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f126a5-af59-4242-b961-055c2d0f037a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def localize_tables_in_sections(pdf, sections):\n",
    "    sections = deepcopy(sections)\n",
    "    # get TOC page number\n",
    "    toc_page_num = get_toc_page(pdf)\n",
    "    # Filter tables beyond TOC page\n",
    "    tables = list(filter(lambda x: x[\"bounding_regions\"][0][\"page_number\"] > toc_page_num, pdf[\"tables\"]))\n",
    "    # Adjust the indices\n",
    "    initial_len = len(pdf[\"tables\"])\n",
    "    final_len = len(tables)\n",
    "    len_diff = initial_len - final_len\n",
    "\n",
    "    # Iterate through each section\n",
    "    for i, section in enumerate(sections):\n",
    "        curr_sec_y = sections[i][\"absolute_bbox\"][0]['y']\n",
    "        subsections = section.get(\"subsections\", [])\n",
    "        # If the sectio containes subsections\n",
    "        if subsections:\n",
    "            for j in range(len(subsections)):\n",
    "                for k, table in enumerate(tables):\n",
    "                    table_first_cell_y = table[\"bounding_regions\"][0][\"absolute_bbox\"][0]['y']\n",
    "\n",
    "                    # Table between main section para and first subsection para\n",
    "                    if (j == 0) and (curr_sec_y < table_first_cell_y < subsections[j][\"absolute_bbox\"][0]['y']):\n",
    "\n",
    "                        sections[i][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "                        continue\n",
    "\n",
    "                    # Table between two subsections para of the same main section\n",
    "                    if (0 < j <= len(subsections)-1) and \\\n",
    "                        (subsections[j-1][\"absolute_bbox\"][0]['y'] < table_first_cell_y < subsections[j][\"absolute_bbox\"][0]['y']):\n",
    "\n",
    "                        sections[i][\"subsections\"][j-1][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "                        continue\n",
    "\n",
    "                    # Table between last subsection para and next main section para\n",
    "                    if (j == len(subsections)-1) and (i != len(sections)-1) and \\\n",
    "                        (subsections[j][\"absolute_bbox\"][0]['y'] < table_first_cell_y < sections[i+1][\"absolute_bbox\"][0]['y']):\n",
    "\n",
    "                        sections[i][\"subsections\"][j][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "                        continue\n",
    "\n",
    "                    if (j == len(subsections)-1) and (i==len(sections)-1) and (subsections[j][\"absolute_bbox\"][0]['y'] < table_first_cell_y):\n",
    "\n",
    "                        sections[i][\"subsections\"][j][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "                        continue\n",
    "        else:\n",
    "            # If section contains no subsection\n",
    "            for k, table in enumerate(tables):\n",
    "                table_first_cell_y = table[\"bounding_regions\"][0][\"absolute_bbox\"][0]['y']\n",
    "\n",
    "                # check if table between two main section\n",
    "                if (i != len(sections)-1) and (curr_sec_y < table_first_cell_y < sections[i+1][\"absolute_bbox\"][0]['y']):\n",
    "                    sections[i][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "                    continue\n",
    "\n",
    "                # check if table in the last main section\n",
    "                if (i == len(sections)-1) and curr_sec_y < table_first_cell_y:\n",
    "                    sections[i][\"tables\"].append(f\"/tables/{k+len_diff}\")\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1990f305-95c7-4206-8c9b-286839af58b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_table_str(df):\n",
    "    # Get column names and data as lists\n",
    "    columns = df.columns.tolist()\n",
    "    if isinstance(columns[0], tuple):\n",
    "        columns = [f\"{ ' - '.join([str(col) for col in col_tuple]) }\" for col_tuple in columns]\n",
    "\n",
    "    data = df.values.tolist()\n",
    "    # Calculate maximum width for each column\n",
    "    max_lengths = [max([len(str(row[i])) for row in data] + [len(str((columns[i])))]) for i in range(len(columns))]\n",
    "    # Create the table string\n",
    "    table_str = ''\n",
    "    # Header row\n",
    "    header = ' | '.join(f'{columns[i]:^{max_lengths[i]}}' for i in range(len(columns)))\n",
    "    table_str += f'| {header} |\\n'\n",
    "    # Separator row\n",
    "    separator = '-+-'.join('-' * max_lengths[i] for i in range(len(columns)))\n",
    "    table_str += f'+-{separator}-+\\n'\n",
    "    # Data rows\n",
    "    for row in data:\n",
    "        table_str += '| ' + ' | '.join(f'{str(item):^{max_lengths[i]}}' for i, item in enumerate(row)) + ' |\\n'\n",
    "        # separate each row by a dash line\n",
    "        table_str += f'+-{separator}-+\\n'  # Move this line out of the loop to only underline the last row\n",
    "\n",
    "    return table_str\n",
    "\n",
    "\n",
    "def replace_text_with_table(pdf, sections):\n",
    "    # Function to sort by bounding box\n",
    "    def sort_by_bounding_box(bbox):\n",
    "        x_coordinates = [corner[\"x\"] for corner in bbox]\n",
    "        y_coordinates = [corner[\"y\"] for corner in bbox]\n",
    "        x_centroid = sum(x_coordinates)/len(x_coordinates)\n",
    "        y_centroid = sum(y_coordinates)/len(y_coordinates)\n",
    "        return (y_coordinates[0], x_coordinates[0])\n",
    "\n",
    "\n",
    "    sections = deepcopy(sections)\n",
    "    for i, section in enumerate(sections):\n",
    "        # check if section contains tables and replace them with markdown\n",
    "        #section[\"content\"].sort(key = lambda x: (x[\"absolute_bbox\"][0]['y'], x[\"absolute_bbox\"][0]['x']))\n",
    "        section[\"content\"].sort(key = lambda x: sort_by_bounding_box(x['absolute_bbox']))\n",
    "        sec_tables = section.get(\"tables\", [])\n",
    "        if sec_tables:\n",
    "            for k, table in enumerate(sec_tables):\n",
    "                table_idx = int( table.split(\"/\")[-1] )\n",
    "                table_bbox = pdf[\"tables\"][table_idx][\"bounding_regions\"][0][\"absolute_bbox\"]\n",
    "                table_first_cell_y1 = pdf[\"tables\"][table_idx][\"cells\"][0][\"bounding_regions\"][0][\"absolute_bbox\"][0]['y']\n",
    "                table_last_cell_y4 = pdf[\"tables\"][table_idx][\"cells\"][-1][\"bounding_regions\"][0][\"absolute_bbox\"][3]['y']\n",
    "                # get the table dataframe\n",
    "                table_df = TableProcessor(pdf[\"tables\"][table_idx]).to_dataframe()\n",
    "\n",
    "                start_index = None\n",
    "                for l, content in enumerate(section[\"content\"], start=0):\n",
    "                    text_bbox_y1 = content[\"absolute_bbox\"][0]['y']\n",
    "                    text_bbox_y4 = content[\"absolute_bbox\"][3]['y']\n",
    "                    if table_first_cell_y1 <= text_bbox_y1 and table_last_cell_y4 > text_bbox_y4:\n",
    "                        start_index = l\n",
    "                        break\n",
    "\n",
    "                end_index = None\n",
    "                if start_index is not None:\n",
    "                    for l, content in enumerate(section[\"content\"][start_index+1:], start=start_index+1):\n",
    "                        text_bbox_y1 = content[\"absolute_bbox\"][0]['y']\n",
    "                        text_bbox_y4 = content[\"absolute_bbox\"][3]['y']\n",
    "                        if (l <= len(section[\"content\"])-1) and text_bbox_y1 > table_last_cell_y4:\n",
    "                            end_index = l-1\n",
    "                            break\n",
    "                        if (l == len(section[\"content\"])-1) and text_bbox_y4 <= table_last_cell_y4:\n",
    "                            end_index = len(section[\"content\"])-1\n",
    "\n",
    "                if (start_index is not None) and (end_index is not None):\n",
    "                    sections[i][\"content\"] = section[\"content\"][:start_index] + \\\n",
    "                                            [{\"text\": dataframe_to_table_str(table_df), \"absolute_bbox\":table_bbox}] + \\\n",
    "                                            section[\"content\"][end_index+1:]\n",
    "\n",
    "        # Check if section has subsections\n",
    "        subsections = section.get(\"subsections\")\n",
    "        if subsections:\n",
    "            for j, subsection in enumerate(subsections):\n",
    "                #subsection[\"content\"].sort(key = lambda x: (x[\"absolute_bbox\"][0]['y'], x[\"absolute_bbox\"][0]['x']))\n",
    "                subsection[\"content\"].sort(key = lambda x: sort_by_bounding_box(x['absolute_bbox']))\n",
    "                # If subsection has tables\n",
    "                subsec_tables = subsection.get(\"tables\", [])\n",
    "                if subsec_tables:\n",
    "                    for k, table in enumerate(subsec_tables):\n",
    "\n",
    "                        table_idx = int( table.split(\"/\")[-1] )\n",
    "                        table_bbox = pdf[\"tables\"][table_idx][\"bounding_regions\"][0][\"absolute_bbox\"]\n",
    "                        table_first_cell_y1 = pdf[\"tables\"][table_idx][\"cells\"][0][\"bounding_regions\"][0][\"absolute_bbox\"][0]['y']\n",
    "                        table_last_cell_y4 = pdf[\"tables\"][table_idx][\"cells\"][-1][\"bounding_regions\"][0][\"absolute_bbox\"][3]['y']\n",
    "                        # get the table dataframe\n",
    "                        table_df = TableProcessor(pdf[\"tables\"][table_idx]).to_dataframe()\n",
    "\n",
    "                        start_index = None\n",
    "                        for l, content in enumerate(subsection[\"content\"], start=0):\n",
    "                            text_bbox_y1 = content[\"absolute_bbox\"][0]['y']\n",
    "                            text_bbox_y4 = content[\"absolute_bbox\"][3]['y']\n",
    "                            if table_first_cell_y1 <= text_bbox_y1 and table_last_cell_y4 > text_bbox_y4:\n",
    "                                start_index = l\n",
    "                                break\n",
    "\n",
    "                        end_index = None\n",
    "                        if start_index is not None:\n",
    "                            for l, content in enumerate(subsection[\"content\"][start_index+1:], start=start_index+1):\n",
    "                                text_bbox_y1 = content[\"absolute_bbox\"][0]['y']\n",
    "                                text_bbox_y4 = content[\"absolute_bbox\"][3]['y']\n",
    "                                if (l <= len(subsection[\"content\"])-1) and text_bbox_y1 > table_last_cell_y4:\n",
    "                                    end_index = l-1\n",
    "                                    break\n",
    "                                if (l == len(subsection[\"content\"])-1) and text_bbox_y4 <= table_last_cell_y4:\n",
    "                                    end_index = len(subsection[\"content\"])-1\n",
    "\n",
    "                        if (start_index is not None) and (end_index is not None):\n",
    "                            sections[i][\"subsections\"][j][\"content\"] = subsection[\"content\"][:start_index] + \\\n",
    "                                                                       [{\"text\": dataframe_to_table_str(table_df), \"absolute_bbox\":table_bbox}] + \\\n",
    "                                                                        subsection[\"content\"][end_index+1:]\n",
    "    return sections\n",
    "\n",
    "\n",
    "\n",
    "def get_sectionwise_text(sections):\n",
    "    text_dict = {}\n",
    "    for section in sections:\n",
    "        text_dict[section[\"name\"]] = {\"content\": \"\"}\n",
    "        if section[\"content\"]:\n",
    "            text_dict[section[\"name\"]][\"content\"] = \"\\u0333\".join(section['name']) + \"\\n\\n\"\n",
    "            for content in section[\"content\"]:\n",
    "                text_dict[section[\"name\"]][\"content\"] += content[\"text\"] + \"\\n\"\n",
    "        if section[\"subsections\"]:\n",
    "            text_dict[section[\"name\"]][\"subsections\"] = {}\n",
    "            for subsection in section[\"subsections\"]:\n",
    "                text_dict[section[\"name\"]][\"subsections\"][subsection[\"name\"]] = \"\\n\" + f\"{subsection['name']}\" + \"\\n\\n\"\n",
    "                for subsection_content in subsection[\"content\"]:\n",
    "                    text_dict[section[\"name\"]][\"subsections\"][subsection[\"name\"]] += subsection_content[\"text\"] + \"\\n\\n\"\n",
    "    return text_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e5e0d11-c586-481c-978a-6aad1c523f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Llama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "467f55bd-07dd-4728-ac78-3c7933f6b7b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Llama\n",
    "class LlamaServer:\n",
    "    '''\n",
    "    This class represnts the instances of hosted LLAMA enpoint\n",
    "    '''\n",
    "    def __init__(self, endpoint: str, api_key: str):\n",
    "        '''\n",
    "        Takes llama endpoint and the api token as input and initializes a LLAMA instance\n",
    "        Args:\n",
    "            enpoint: Endpoint of hosted LLAMA\n",
    "            api_key: Access token to query the LLAMA endpoint\n",
    "        '''\n",
    "        self.llama_enpoint = endpoint\n",
    "        self.llama_key = api_key\n",
    "\n",
    "    def chat_completion(self, messages, **gen_config):\n",
    "        '''\n",
    "        Chat with the hosted LLAMA. Takes input as a list of messages to query the llama and returns the response obtained\n",
    "        Args:\n",
    "            messages: A list of messages to query hosted LLAMA\n",
    "            **gen_config: Generation parameters for LLAMA\n",
    "        Returns:\n",
    "            response: Response from the LLMA server\n",
    "        '''\n",
    "        status =\"\"\n",
    "        try:\n",
    "            llama_headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authentication\": f\"Bearer {self.llama_key}\"\n",
    "            }\n",
    "            llama_data = {\n",
    "                \"messages\": messages,\n",
    "                **gen_config\n",
    "            }\n",
    "            llama_response_obj = requests.post(self.llama_enpoint, headers=llama_headers, data=json.dumps(llama_data))\n",
    "            if llama_response_obj.status_code == requests.codes.ok:\n",
    "                status = \"Success\"\n",
    "                llama_response = llama_response_obj.json()['choices'][0]['message']['content']\n",
    "                return self._fix_json_response(llama_response)\n",
    "        except Exception as e:\n",
    "            status = f\"An error occurred while queryring llama3 serving endpoint: {e}\"\n",
    "            return None\n",
    "\n",
    "    def _fix_json_response(self, response: str) -> dict:\n",
    "        \"\"\"\n",
    "        Fixes common JSON formatting issues in a string response.\n",
    "\n",
    "        Args:\n",
    "            response (str): The response string from LLAMA.\n",
    "\n",
    "        Returns:\n",
    "            dict: The JSON-compatible dictionary.\n",
    "        \"\"\"\n",
    "        # Attempt to parse the JSON without any modifications\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            pass  # If it fails, continue with the processing steps\n",
    "\n",
    "        # Remove markdown JSON code fences and the `json` keyword\n",
    "        response = re.sub(r'```json\\n|```|json', '', response)\n",
    "        # Replace non-standard quotes with standard double quotes\n",
    "        response = re.sub(r'“', '\"', response)\n",
    "        response = re.sub(r'”', '\"', response)\n",
    "        response = re.sub(r\"'\", \"\\\"\", response)\n",
    "        response = re.sub(r\"'\", \"\\\"\", response)\n",
    "        # Replace invalid fractions with their approximate decimal equivalents\n",
    "        response = re.sub(r'(\\d+)/(\\d+)', lambda m: str(float(m.group(1)) / float(m.group(2))), response)\n",
    "        # Strip leading and trailing whitespace\n",
    "        response = response.strip()\n",
    "        # Attempt to find JSON object or array within the string\n",
    "        match = re.search(r'\\{[\\s\\S]*\\}|\\[[\\s\\S]*\\]', response)\n",
    "\n",
    "        if match:\n",
    "            cleaned_string = match.group(0)\n",
    "        else:\n",
    "            # If no JSON object or array is found, assume the whole response needs fixing\n",
    "            cleaned_string = response\n",
    "\n",
    "        # Count the number of opening and closing braces\n",
    "        open_curly = cleaned_string.count('{')\n",
    "        close_curly = cleaned_string.count('}')\n",
    "        open_square = cleaned_string.count('[')\n",
    "        close_square = cleaned_string.count(']')\n",
    "\n",
    "        # Attempt to add enclosing brackets if missing\n",
    "        if open_curly == 1 and close_curly == 0:\n",
    "            cleaned_string += '}'\n",
    "        elif close_curly == 1 and open_curly == 0:\n",
    "            cleaned_string = '{' + cleaned_string\n",
    "        elif open_square == 1 and close_square == 0:\n",
    "            cleaned_string += ']'\n",
    "        elif close_square == 1 and open_square == 0:\n",
    "            cleaned_string = '[' + cleaned_string\n",
    "\n",
    "        # Handle case where both opening and closing brackets are missing\n",
    "        if open_curly == 0 and close_curly == 0 and open_square == 0 and close_square == 0:\n",
    "            cleaned_string = '{' + cleaned_string + '}'\n",
    "\n",
    "        # Attempt to fix common issues and parse the JSON\n",
    "        try:\n",
    "            return json.loads(cleaned_string)\n",
    "        except json.JSONDecodeError:\n",
    "            # Handle common issues\n",
    "            cleaned_string = cleaned_string.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "            cleaned_string = cleaned_string.replace(\"\\n\", \" \")  # Remove newlines\n",
    "            cleaned_string = cleaned_string.replace(\"\\t\", \" \")  # Remove tabs\n",
    "\n",
    "            try:\n",
    "                return json.loads(cleaned_string)\n",
    "            except json.JSONDecodeError:\n",
    "                try:\n",
    "                    wrapped_string = f\"[{cleaned_string}]\"\n",
    "                    return json.loads(wrapped_string)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Unable to fix JSON response from LLM. The response:\\n\", response)\n",
    "                    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f94704dd-252e-41e2-8df1-f88861048ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Chat prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf682744-245d-4910-965b-59bcb6c0e852",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prompts for attribute extraction\n",
    "class ChatPrompt:\n",
    "    '''\n",
    "    This class contains different prompts to query LLAMA server in order to extract different information.\n",
    "    '''\n",
    "    def _split_evaluation_and_adverse_event_text(self, section_text):\n",
    "        with StringIO(section_text) as f:\n",
    "            text_lines = f.readlines()\n",
    "        i = 0\n",
    "        for i, line in enumerate(text_lines):\n",
    "            if re.match(r'^[\\d#].*\\.\\d+', line) and \"adverse events\" in line.strip().lower():\n",
    "                adverse_event_section_ix = i\n",
    "                break\n",
    "\n",
    "        eval_text = []\n",
    "        for j in range(i):\n",
    "            if text_lines[j].startswith(\"<!--\") or text_lines[j].lower().startswith(\"docusign\"):\n",
    "                continue\n",
    "            if text_lines[j].startswith(\"<\") or text_lines[j].startswith(\"!\"):\n",
    "                continue\n",
    "            eval_text.append(text_lines[j])\n",
    "\n",
    "        evaluation_text = \"\".join(eval_text)\n",
    "        remaining_text = \"\".join(text_lines[i:])\n",
    "\n",
    "        return evaluation_text.strip(), remaining_text.strip()\n",
    "\n",
    "\n",
    "    # def _get_panelist_accountability_text(self, section_text):\n",
    "    #     parent_key = None\n",
    "    #     candidate_key = None\n",
    "    #     is_sub_key = False\n",
    "    #     is_parent_key = False\n",
    "    #     for key in section_text.keys():\n",
    "    #         if bool(re.match(r'panelist accountability', key, re.IGNORECASE)):\n",
    "    #             is_parent_key = True\n",
    "    #             candidate_key = key\n",
    "    #             break\n",
    "    #         if bool(re.match(r'panelist disposition and demographics', key, re.IGNORECASE)):\n",
    "    #             parent_key = key\n",
    "    #             subsections = section_text[parent_key].get(\"subsections\", [])\n",
    "    #             if subsections:\n",
    "    #                 for sub_key in section_text[parent_key][\"subsections\"].keys():\n",
    "    #                     if bool(re.search(r'panelist accountability', sub_key, re.IGNORECASE)):\n",
    "    #                         is_sub_key = True\n",
    "    #                         candidate_key = sub_key\n",
    "    #                         break\n",
    "    #             else:\n",
    "    #                 continue\n",
    "\n",
    "\n",
    "    #     if not candidate_key:\n",
    "    #         for key in section_text.keys():\n",
    "    #             if bool(re.match(r'panelist disposition and demographic', key, re.IGNORECASE)):\n",
    "    #                 candidate_key = key\n",
    "    #         mainsec =  section_text.get(candidate_key, \"\")\n",
    "    #         if mainsec:\n",
    "    #             return mainsec.get(\"content\", \"\")\n",
    "\n",
    "    #     if is_parent_key:\n",
    "    #         return section_text[candidate_key].get(\"content\", \"\")\n",
    "    #     elif is_sub_key:\n",
    "    #         return section_text[parent_key][\"subsections\"][candidate_key]\n",
    "    #     else:\n",
    "    #         return \"\"\n",
    "    def _get_adverse_events(self, section_text):\n",
    "        parent_key = None\n",
    "        candidate_key = None\n",
    "        is_sub_key = False\n",
    "        is_parent_key = False\n",
    "        text = \"\"\n",
    "        for key in section_text.keys():\n",
    "            if bool(re.match(r'adverse event', key, re.IGNORECASE)):\n",
    "                if isinstance(section_text[key], dict):\n",
    "                    if \"subsections\" in section_text[key].keys():\n",
    "                        for k in section_text[key][\"subsections\"]:\n",
    "                            text += section_text[key][\"subsections\"][k] + \"\\n\\n\"\n",
    "                    else:\n",
    "                        text += section_text[key][\"content\"] + \"\\n\"\n",
    "\n",
    "            if not text and bool(re.match(r'study plan', key, re.IGNORECASE)):\n",
    "                subsections = section_text[key].get(\"subsections\", {})\n",
    "                if subsections:\n",
    "                    for sub_key in section_text[key][\"subsections\"].keys():\n",
    "                        if bool(re.search(r'adverse event', sub_key, re.IGNORECASE)):\n",
    "                            text += section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "                else:\n",
    "                    continue\n",
    "        return text\n",
    "\n",
    "    def _get_inclusion_criteria(self, section_text):\n",
    "        parent_key = None\n",
    "        candidate_key = None\n",
    "        is_sub_key = False\n",
    "        is_parent_key = False\n",
    "        text = \"\"\n",
    "        for key in section_text.keys():\n",
    "            if bool(re.match(r'inclusion criteria', key, re.IGNORECASE)):\n",
    "                if isinstance(section_text[key], dict):\n",
    "                    if \"subsections\" in section_text[key].keys():\n",
    "                        for k in section_text[key][\"subsections\"]:\n",
    "                            text += section_text[key][\"subsections\"][k] + \"\\n\\n\"\n",
    "                    else:\n",
    "                        text += section_text[key][\"content\"] + \"\\n\"\n",
    "\n",
    "            if not text and bool(re.match(r'study panel', key, re.IGNORECASE)):\n",
    "                subsections = section_text[key].get(\"subsections\", {})\n",
    "                if subsections:\n",
    "                    for sub_key in section_text[key][\"subsections\"].keys():\n",
    "                        if bool(re.search(r'inclusion criteria', sub_key, re.IGNORECASE)):\n",
    "                            text += section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "                else:\n",
    "                    continue\n",
    "        return text\n",
    "\n",
    "\n",
    "    def _get_evaluation(self, section_text):\n",
    "        parent_key = None\n",
    "        candidate_key = None\n",
    "        is_sub_key = False\n",
    "        is_parent_key = False\n",
    "        text = \"\"\n",
    "        for key in section_text.keys():\n",
    "            if bool(re.match(r'evaluation', key, re.IGNORECASE)):\n",
    "                if isinstance(section_text[key], dict):\n",
    "                    if \"subsections\" in section_text[key].keys():\n",
    "                        for k in section_text[key][\"subsections\"]:\n",
    "                            text += section_text[key][\"subsections\"][k] + \"\\n\\n\"\n",
    "                    else:\n",
    "                        text += section_text[key][\"content\"] + \"\\n\"\n",
    "\n",
    "            if not text and bool(re.match(r'study plan', key, re.IGNORECASE)):\n",
    "                subsections = section_text[key].get(\"subsections\", {})\n",
    "                if subsections:\n",
    "                    for sub_key in section_text[key][\"subsections\"].keys():\n",
    "                        if bool(re.search(r'evaluation', sub_key, re.IGNORECASE)):\n",
    "                            print('getting text:')\n",
    "                            text += section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "                else:\n",
    "                    continue\n",
    "        return text\n",
    "\n",
    "    def _get_panelist_accountability_text(self, section_text):\n",
    "        parent_key = None\n",
    "        candidate_key = None\n",
    "        is_sub_key = False\n",
    "        is_parent_key = False\n",
    "        text = \"\"\n",
    "        for key in section_text.keys():\n",
    "            if bool(re.match(r'panelist disposition', key, re.IGNORECASE)):\n",
    "                if isinstance(section_text[key], dict):\n",
    "                    if \"subsections\" in section_text[key].keys():\n",
    "                        for k in section_text[key][\"subsections\"]:\n",
    "                            text += section_text[key][\"subsections\"][k] + \"\\n\\n\"\n",
    "                    else:\n",
    "                        text += section_text[key][\"content\"] + \"\\n\"\n",
    "\n",
    "            if not text and bool(re.match(r'study panel', key, re.IGNORECASE)):\n",
    "                subsections = section_text[key].get(\"subsections\", {})\n",
    "                if subsections:\n",
    "                    for sub_key in section_text[key][\"subsections\"].keys():\n",
    "                        if bool(re.search(r'panelist disposition', sub_key, re.IGNORECASE)):\n",
    "                            text += section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        # if not text:\n",
    "        #     for key in section_text.keys():\n",
    "        #         if bool(re.match(r'panelist disposition and demographic', key, re.IGNORECASE)):\n",
    "        #             if isinstance(section_text[key], dict) and \"subsections\" in section_text[key].keys():\n",
    "        #                 for sub_key in section_text[key][\"subsections\"].keys():\n",
    "        #                     text + section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "        #             else:\n",
    "        #                 text += section_text[key][\"content\"] + \"\\n\"\n",
    "\n",
    "        # if not text:\n",
    "        #     for key in section_text.keys():\n",
    "        #         if bool(re.match(r'panelist withdrawal', key, re.IGNORECASE)):\n",
    "        #             if isinstance(section_text[key], dict) and \"subsections\" in section_text[key].keys():\n",
    "        #                 for sub_key in section_text[key][\"subsections\"].keys():\n",
    "        #                     if re.match(r\"panelist accountability\", sub_key, re.IGNORECASE):\n",
    "        #                         text += section_text[key][\"subsections\"][sub_key] + \"\\n\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    def skin_type_prompt(self, sectionwise_text):\n",
    "        text = self._get_inclusion_criteria(sectionwise_text)\n",
    "        demographics = '''Extract all unique mentions of skin types values mentioned as a text  and return it as a list under key Skin_Type.\n",
    "        Ensure the following restrictions when picking skin types:\n",
    "        1. Skin type is not fitzpatrick type\n",
    "        2. Skin type is not skin tones\n",
    "        3. skin type is not test site\n",
    "        4. skin type is not ethnicity\n",
    "\n",
    "        If the skin type is not found, please return an empty list.\n",
    "\n",
    "        Return the answer strictly in following JSON format with following key:\n",
    "        1. Skin_Type\n",
    "\n",
    "        If no text is provided, please return both the keys as null.\n",
    "        '''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text about a study that contains panelist information:\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "    def instrument_prompt(self, sectionwise_text):\n",
    "        print(\"inside instrument prompt\")\n",
    "        text = self._get_evaluation(sectionwise_text)\n",
    "\n",
    "        demographics = '''Please extract the instrument used from the above text and return it in JSON format with key 'instrument'. If the instrument is not found, please return both the key as null.\n",
    "\n",
    "        Return the answer strictly in following JSON format:\n",
    "        {\"instrument\": list of name of instruments used}\n",
    "\n",
    "        If no text is provided, please return both the keys as null.'''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text about a study that contains tables:\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n",
    "\n",
    "    def fitzpatrick_skin_lip_prompt(self, sectionwise_text):\n",
    "        '''\n",
    "        Returns a chat prompt to query LLAMA to extract - Fitzpatrick type, skin tyoe and lip type\n",
    "        '''\n",
    "        text = self._get_panelist_accountability_text(sectionwise_text)\n",
    "\n",
    "        constraints = []\n",
    "        demographics = \"\"\"\n",
    "        Give me the following information from the above text\n",
    "        {fp_prompt}\n",
    "        {skin_prompt}\n",
    "        {lip_prompt}\n",
    "\n",
    "        Your JSON should have the following keys:\n",
    "        {fp_key}\n",
    "        {skin_key}\n",
    "        {lip_key}\n",
    "        \"\"\"\n",
    "\n",
    "        counter = 0\n",
    "        if \"fitzpatrick\" in text.lower():\n",
    "            counter += 1\n",
    "            fp_key = f\"{counter}. Fitzpatrick_Type\"\n",
    "            fp_var = f\"{counter}. Extract all unique mentions of Fitzpatrick skin types mentioned in the respective markdown format content and return it as a list under key Fitzpatrick_Type.\"\n",
    "            constraints.append(\"Ensure that the extraction for Fitzpatrick type focuses solely on Fitzpatrick type names and does not include Skin types or any other classifications.\")\n",
    "        else:\n",
    "            fp_var = f\"\"\n",
    "            fp_key = f\"\"\n",
    "        if \"skin\" in text.lower():\n",
    "            counter += 1\n",
    "            skin_key = f\"{counter}. Skin_Tone\"\n",
    "            skin_var = f\"{counter}. Extract all unique mentions of skin tone values mentioned in the respective markdown format content and return it as a list under key Skin_Tone.\"\n",
    "            constraints.append( \"Ensure that the extraction for Skin tone focuses solely on skin tone names and does not include Fitzpatrick types, skin type(normal, dry, oily) or any other classifications.\")\n",
    "        else:\n",
    "            skin_var = f\"\"\n",
    "            skin_key = f\"\"\n",
    "        if \"lip\" in text.lower():\n",
    "            counter += 1\n",
    "            lip_key = f\"{counter}. Lip_Type\"\n",
    "            lip_var = f\"{counter}. Find all unique mentions of lip types mentioned on the respective markdown format content and return it as a list under key Lip_Type.\"\n",
    "            constraints.append(\"Ensure that the extraction for Lip type focuses solely on Lip type names and does not include Skin types or any other classifications.\")\n",
    "        else:\n",
    "            lip_var = f\"\"\n",
    "            lip_key = f\"\"\n",
    "        demographics = demographics.format(\n",
    "            skin_prompt=skin_var,\n",
    "            fp_prompt=fp_var,\n",
    "            lip_prompt=lip_var,\n",
    "            skin_key=skin_key,\n",
    "            fp_key=fp_key,\n",
    "            lip_key=lip_key,\n",
    "        )\n",
    "\n",
    "        for i in range(counter):\n",
    "            demographics += \"\\n\" + constraints[i]\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below are some tables. Extract the attributes from their respective tables.\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n",
    "\n",
    "    def number_enrolled_completed_prompt(self, sectionwise_text):\n",
    "        '''\n",
    "        Returns a chat prompt to query LLAMA to extract - Number Enrolled and Number Completed\n",
    "        '''\n",
    "        text = self._get_panelist_accountability_text(sectionwise_text)\n",
    "\n",
    "        demographics = '''From the above text,\n",
    "        Please extract the total number of panelists who enrolled in the study.\n",
    "        Also, extract the total number of panelists who completed the study.\n",
    "\n",
    "        Please return the answer in following JSON format:\n",
    "        {\"Number_Enrolled\": total Number of panelists who enrolled in the study, \"Number_Completed\": total Number of panelists who completed the study}\n",
    "\n",
    "        if any of the numbers are not found, specify it as null.\n",
    "        If no text is provided. Return the value for both the numbers as null'''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text about a study that contains tables:\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n",
    "\n",
    "    def age_range_prompt(self, sectionwise_text):\n",
    "        '''\n",
    "        Returns a chat prompt to query LLAMA to extract - Minimum and maximum ages\n",
    "        '''\n",
    "        text = self._get_panelist_accountability_text(sectionwise_text)\n",
    "\n",
    "        demographics = '''Please extract the age range of panelists from the above text and return it in JSON format as minimum-maximum under the keys 'min_age' and 'max_age'. If the age range is not found, please return both the keys as null.\n",
    "\n",
    "        Please return the answer in following JSON format:\n",
    "        {\"min_age\": minimum age, \"max_age\": maximum age}\n",
    "\n",
    "        If no text is provided, please return both the keys as null.'''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text about a study that contains tables:\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n",
    "    def ethnicity_prompt(self, sectionwise_text):\n",
    "        '''\n",
    "        Returns a chat prompt to query LLAMA to extract - list of ethnicities\n",
    "        '''\n",
    "        text = self._get_panelist_accountability_text(sectionwise_text)\n",
    "\n",
    "        demographics = '''From the above text,\n",
    "        Please extract all unique mentions of ethnicities (e.g., Asian, Hispanic) from the text and return them as a JSON object with an list under the key 'Ethnicity'\n",
    "        (provide the ethnicity names as mentioned in the text and exlcude the count of panelists).\n",
    "\n",
    "        Please return the answer in following JSON format:\n",
    "         {\"Ethnicity\": list of Ethnicities}\n",
    "\n",
    "        Here is an example of the response should look like: {\"Ethnicity\": [\"Hispanic\", \"Indian\", \"Black African/American\"] ...}\n",
    "\n",
    "        Please ensure that the extraction focuses solely on ethnicity names and does not include Fitzpatrick skin types, skin tones or any other classifications.\n",
    "\n",
    "        If no ethnicities are found, please ensure that the JSON output still includes the 'Ethnicity' key with an empty list.\n",
    "        If no text is provided, please ensure that the JSON output still includes the 'Ethnicity' key with an empty list.'''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, who can read some given content and extract attributes from that content in JSON format. You are only capable of responding in JSON format and nothing else\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text about a study that contains tables:\\n\\n{text}\\n\\n{demographics}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def adverse_event_attribute_prompt(self, sectionwise_text):\n",
    "        '''\n",
    "        Returns a chat prompt to query LLAMA and extract - Number of Adverse events\n",
    "        '''\n",
    "        adverse_event_text = self._get_adverse_events(sectionwise_text)\n",
    "        print('adverse event text:',adverse_event_text)\n",
    "\n",
    "\n",
    "        # if isinstance(sectionwise_text, dict):\n",
    "        #     result_key = None\n",
    "        #     adverse_key = None\n",
    "        #     for i in sectionwise_text.keys():\n",
    "        #         if bool(re.search(r'study plan',i,re.IGNORECASE)):\n",
    "        #             result_key = i\n",
    "        #             if len(sectionwise_text[result_key].get('subsections',{}))>0:\n",
    "        #                 for j in sectionwise_text[result_key]['subsections']:\n",
    "        #                     if bool(re.search(r'adverse events',j,re.IGNORECASE)):\n",
    "        #                         adverse_key = j\n",
    "        #                         adverse_event_text = sectionwise_text[result_key]['subsections'][adverse_key]\n",
    "        #                         break\n",
    "        #                 break\n",
    "\n",
    "        # if not adverse_event_text:\n",
    "        #     adverse_event_main_sec = None\n",
    "        #     for key in sectionwise_text.keys():\n",
    "        #         if bool(re.search(r\"adverse event\", key, re.IGNORECASE)) and \\\n",
    "        #             (bool(re.search(r\"discomfort mention\", key, re.IGNORECASE)) or\n",
    "        #              bool(re.search(r\"mentions of discomfort\", key, re.IGNORECASE))):\n",
    "        #                 adverse_event_main_sec = key\n",
    "        #                 break\n",
    "        #         else:\n",
    "        #             if bool(re.search(r'results',i,re.IGNORECASE)):\n",
    "        #                 adverse_event_main_sec = i\n",
    "        #     if adverse_event_main_sec:\n",
    "        #         adverse_event_text = sectionwise_text[adverse_event_main_sec].get(\"content\", \"\")\n",
    "\n",
    "\n",
    "        inst = '''Please extract the number of Adverse Events in the above text.\n",
    "        Adverse Events represents the number of  panelists who reported discomfort or irritation or adverse reaction to the test product.\n",
    "\n",
    "        Please return the answer in following JSON format:\n",
    "        {\"Adverse_Events\": count of Adverse Events}.'''\n",
    "\n",
    "        messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful AI assistant, an expert at extracting information accurately from the provided text and tables. You are designed to output JSON only (with the right syntax) and nothing else. \"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Given below is a text that contains tables:\\n\\n{adverse_event_text}\\n\\n{inst}\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"\"\n",
    "                }\n",
    "            ]\n",
    "        return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c00be35-3ba3-4436-a014-ae5d5ac4563e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Table Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "248818d0-d522-4bff-a8d1-6a23f4d75266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process tables\n",
    "class TableProcessor:\n",
    "    '''\n",
    "    Contains methods to process a table obtained from Azure Document Inteliigence response\n",
    "    '''\n",
    "    def __init__(self, table):\n",
    "        '''\n",
    "        Iniatialize the class with the table obtained from Azure Document Inteliigence response\n",
    "        Args:\n",
    "            table:  Table to be processed\n",
    "        '''\n",
    "        self.cells = table[\"cells\"]\n",
    "        self.n_rows = table[\"row_count\"]\n",
    "        self.n_cols = table[\"column_count\"]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    def to_dataframe(self):\n",
    "        '''\n",
    "        Converts the table to pandas dataframe\n",
    "        '''\n",
    "        col_levels = self._get_column_levels()\n",
    "        vals = [cell[\"content\"] if cell else None for cell in self.replicate_cells_spanwise()]\n",
    "        #vals_arr = np.array(vals).reshape(n_rows, n_cols)\n",
    "        vals_arr = [[vals[i*self.n_cols + j] for j in range(self.n_cols)] for i in range(self.n_rows)]\n",
    "\n",
    "        df = pd.DataFrame(vals_arr)\n",
    "\n",
    "        if col_levels == 1:\n",
    "            df.columns = df.iloc[0]\n",
    "            df.drop(df.index[:col_levels], axis = 0, inplace=True)\n",
    "            df = df.reset_index().drop(columns=[\"index\"], axis=1)\n",
    "            if not df.columns[0]:\n",
    "                df.columns = [\"<rowHeader>\"] + df.columns[1:].tolist()\n",
    "\n",
    "        if col_levels > 1:\n",
    "            col_arr = [df.iloc[i].tolist() for i in range(col_levels)]\n",
    "            for i in range(col_levels):\n",
    "                if not col_arr[i][0]:\n",
    "                    col_arr[i][0] = \"<rowHeader>\"\n",
    "            df.columns = pd.MultiIndex.from_arrays(col_arr, names = [None]*col_levels)\n",
    "            df.drop(df.index[:col_levels], axis = 0, inplace=True)\n",
    "            if not df.columns.is_monotonic_increasing:\n",
    "                df = df.sort_index(axis=1)\n",
    "            df = df.reset_index().drop(columns=[\"index\"], axis=1, level=0)\n",
    "        return df\n",
    "\n",
    "    #Converts the rows of the table into a list of json objects\n",
    "    def to_json(self):\n",
    "        '''\n",
    "        Conerts the table into a list of json object whose each element representing a row in the table\n",
    "        '''\n",
    "        df = self.to_dataframe()\n",
    "        return json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "\n",
    "    def replicate_cells_spanwise(self):\n",
    "        '''\n",
    "        Exapnds the table cells to match ( num_rows*num_columns ) in the table\n",
    "        '''\n",
    "        cells_new = [None for _ in range(self.n_rows*self.n_cols)]\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            r, c = cell[\"row_index\"], cell[\"column_index\"]\n",
    "            # row and column span of the cell\n",
    "            row_span = int(cell.get(\"row_span\", 1))\n",
    "            col_span = int(cell.get(\"column_span\", 1))\n",
    "\n",
    "            cells_new[r*self.n_cols+c] = cell\n",
    "            # If mulitple row span - expand along the row\n",
    "            if row_span and row_span > 1:\n",
    "                for rs in range(1, row_span):\n",
    "                    cells_new[(r+rs)*self.n_cols+c] = cell\n",
    "            # if multiple column span - expand along the column\n",
    "            if col_span and col_span > 1:\n",
    "                for cs in range(1, col_span):\n",
    "                    cells_new[c+cs] = cell\n",
    "\n",
    "        return cells_new\n",
    "\n",
    "    # Gets the number of rows as column header from given table cells\n",
    "    def _get_column_levels(self):\n",
    "        '''\n",
    "        Function to determine the number of column levels in the table\n",
    "        '''\n",
    "        row_indices = []\n",
    "        header_cells = [cell for cell in self.cells if cell and cell.get(\"kind\", None) == \"columnHeader\"]\n",
    "        for cell in header_cells:\n",
    "            row_indices.append(cell[\"row_index\"])\n",
    "\n",
    "        row_indices.sort()\n",
    "        row_indices = list(set(row_indices))\n",
    "        i=0\n",
    "        for i in range(len(row_indices)):\n",
    "            if i != row_indices[i]:\n",
    "                break\n",
    "        if i == 0 or (i == len(row_indices)-1 and i == row_indices[i]):\n",
    "            i += 1\n",
    "\n",
    "        num_levels = len(row_indices[:i])\n",
    "        return num_levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f178b2-05ec-4732-bbec-d5df44c2912b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableAttributesExtractor:\n",
    "    def get_results_section(self, sections):\n",
    "        for section in sections:\n",
    "            if section['name'] == 'RESULTS' or section['name'] == 'RESULT':\n",
    "                sub_sections = section.get(\"subsections\")\n",
    "                return section,sub_sections\n",
    "\n",
    "    def get_tables_to_main_section(self, section, sub_sections):\n",
    "        if len(sub_sections)>0:\n",
    "            for subsection in sub_sections:\n",
    "                tables = subsection['tables']\n",
    "                section['tables'].extend(tables)\n",
    "        return section\n",
    "\n",
    "    def get_table_header(self, table, pdf):\n",
    "        absolute_bbox = table['cells'][0]['bounding_regions'][0]['absolute_bbox']\n",
    "        cell_content = table['cells'][0]['content']\n",
    "        paragraphs = pdf['paragraphs']\n",
    "        para_list = []\n",
    "        for para in paragraphs:\n",
    "            para_content = para['content']\n",
    "            para_absolute_bbox = para['bounding_regions'][0]['absolute_bbox']\n",
    "            if para_absolute_bbox == absolute_bbox:\n",
    "                break\n",
    "            para_list.append(para)\n",
    "        distance_from_first_cell = []\n",
    "        for para in para_list:\n",
    "            para_absolute_bbox = para['bounding_regions'][0]['absolute_bbox']\n",
    "            max_y_para = np.max([para_absolute_bbox[2][\"y\"], para_absolute_bbox[3][\"y\"]]).item()\n",
    "            min_y_cell = np.min([absolute_bbox[0][\"y\"],absolute_bbox[1][\"y\"]]).item()\n",
    "            distance_from_first_cell.append((min_y_cell - max_y_para))\n",
    "        if len(distance_from_first_cell)!=0:\n",
    "            para_index_for_table_heading = distance_from_first_cell.index(np.min(distance_from_first_cell).item())\n",
    "            return para_list[para_index_for_table_heading]['content']\n",
    "        else:\n",
    "            return \"Not Available\"\n",
    "\n",
    "    def get_dataframe_from_tables(self, tables, section, pdf,results_section_columns,df_clinical_mapping):\n",
    "        df = pd.DataFrame()\n",
    "        results_mapping_dictionary = df_clinical_mapping.set_index(\"RESULTS\")[\"Attribute Mapped Results\"].to_dict()\n",
    "        for table in tables:\n",
    "            table_number = int(table.split(\"/\")[-1])\n",
    "            table_df = TableProcessor(pdf[\"tables\"][table_number]).to_dataframe()\n",
    "            table_header = self.get_table_header(pdf[\"tables\"][table_number],pdf)\n",
    "            table_header = re.sub(\"[^a-zA-Z\\s]\", \"\",table_header).strip()\n",
    "            print(table_header)\n",
    "            l1 = []\n",
    "            for column in table_df.columns:\n",
    "                for row1 in range(len(table_df[column])):\n",
    "                    l = []\n",
    "                    if isinstance(column,str):\n",
    "                        m = column.lower().strip(\"*: \")\n",
    "                        if m in results_section_columns:\n",
    "                            l.extend([results_mapping_dictionary[m],table_df[column][row1],row1,table_number,section['name'],table_header])\n",
    "                            l1.append(l)\n",
    "                    elif isinstance(column,tuple):\n",
    "                        pass\n",
    "                        # l.extend(list(column))\n",
    "                        # l.extend([table_df[column][row1],row1,table_number,section['name']])\n",
    "                        # l1.append(l)\n",
    "            df1 = pd.DataFrame(l1,columns=[\"Attribute_Name\",\"Attribute_Value\",\"Row_Number\",\"Table_Number\",\"Section_Name\",\"Table_Header\"])\n",
    "            df = pd.concat([df,df1], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def get_results_attributes(self, df):\n",
    "        df = df.copy()\n",
    "        dfs = []\n",
    "        table_nums = df[\"Table_Number\"].unique().tolist()\n",
    "        for table_num in table_nums:\n",
    "            temp_df = df[df[\"Table_Number\"] == table_num]\n",
    "            table_dict = {}\n",
    "            columns = temp_df[\"Attribute_Name\"].unique().tolist()\n",
    "            for col in columns:\n",
    "                table_dict[col] = temp_df.loc[temp_df[\"Attribute_Name\"] == col, \"Attribute_Value\"].tolist()\n",
    "            dfs.append(pd.DataFrame(table_dict))\n",
    "\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def results_tables(self, pdf, sections):\n",
    "        section, sub_sections = self.get_results_section(sections)\n",
    "        section = self.get_tables_to_main_section(section,sub_sections)\n",
    "        tables = section['tables']\n",
    "        df = self.get_dataframe_from_tables(tables, section, pdf,results_section_columns,df_clinical_mapping)\n",
    "        df = self.get_results_attributes(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a35002-36a2-4602-abb8-b21adae984c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AttributeExtractor:\n",
    "    '''\n",
    "    This class contains methods to extract various required attributes.\n",
    "    '''\n",
    "    def __init__(self, llm: LlamaServer, chat_prompt: ChatPrompt, gen_config:dict):\n",
    "        '''\n",
    "        Initializes the instance with inputs required to extract the atrributes\n",
    "        Args:\n",
    "            llm: LLAMA instance\n",
    "            chat_prompt: contains chat prompts to query the llm\n",
    "            gen_config: a dictionary of parameters to configure llm generation\n",
    "        '''\n",
    "        self.llm = llm\n",
    "        self.chat_prompt = chat_prompt\n",
    "        self.gen_config = gen_config\n",
    "\n",
    "\n",
    "    def results_attributes(self, pdf, sections):\n",
    "        '''\n",
    "        processes the list of dataframes of results tables and returns a final concatenated dataframe\n",
    "        Args:\n",
    "            pdf: json object containing tables and paragraphs of text\n",
    "        Returns:\n",
    "            A concatenated dataframe of all results tables\n",
    "        '''\n",
    "        table_extractor = TableAttributesExtractor()\n",
    "        df = table_extractor.results_tables(pdf,sections)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def adverse_events(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns adverse events count\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            adverse_event_count: a dictionary wit keys - \"Adverse_Events\" and values - count of adverse events OR None\n",
    "        '''\n",
    "        messages = self.chat_prompt.adverse_event_attribute_prompt(sectionwise_text)\n",
    "        # print(messages)\n",
    "        adverse_event_count = None\n",
    "        try:\n",
    "            adverse_event_count = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"Adverse event exception:\", e)\n",
    "            pass\n",
    "        return adverse_event_count\n",
    "\n",
    "\n",
    "    def number_enrolled_completed_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns number enrolled and completed\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            enrolled_completed: a dictionary containing number enrolled and number completed\n",
    "        '''\n",
    "        messages = self.chat_prompt.number_enrolled_completed_prompt(sectionwise_text)\n",
    "        enrolled_completed = None\n",
    "        try:\n",
    "            enrolled_completed = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"Number enrolled, completed Exception:\", e)\n",
    "            pass\n",
    "        return enrolled_completed\n",
    "\n",
    "    def instrument_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns age range\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            ages: a dictionary containing minimum and maximum age\n",
    "        '''\n",
    "        messages = self.chat_prompt.instrument_prompt(sectionwise_text)\n",
    "        print(messages)\n",
    "        instrument = None\n",
    "        try:\n",
    "            instrument = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"Instrument Exception:\", e)\n",
    "            pass\n",
    "        return instrument\n",
    "\n",
    "    def skin_type_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns age range\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            ages: a dictionary containing minimum and maximum age\n",
    "        '''\n",
    "        messages = self.chat_prompt.skin_type_prompt(sectionwise_text)\n",
    "        print(messages)\n",
    "        skin_type = None\n",
    "        try:\n",
    "            skin_type = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"skin_type Exception:\", e)\n",
    "            pass\n",
    "        return skin_type\n",
    "\n",
    "    def age_range_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns age range\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            ages: a dictionary containing minimum and maximum age\n",
    "        '''\n",
    "        messages = self.chat_prompt.age_range_prompt(sectionwise_text)\n",
    "        ages = None\n",
    "        try:\n",
    "            ages = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"Age Range Exception:\", e)\n",
    "            pass\n",
    "        return ages\n",
    "\n",
    "\n",
    "    def ethnicity_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns dict of list of ethnicities\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            ethnicity: a dictionary containing list of ethnicites under the key \"Ethnicity\"\n",
    "        '''\n",
    "        messages = self.chat_prompt.ethnicity_prompt(sectionwise_text)\n",
    "        ethnicity = None\n",
    "        try:\n",
    "            ethnicity = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"Ethnicity Exception:\", e)\n",
    "            pass\n",
    "        return ethnicity\n",
    "\n",
    "\n",
    "    def fitzpatrick_skin_lip_attr(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns dict of lists of Fitzpatrick types, skin types and lip types\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            fitzpatrick_skin_lip: a dictionary containing lists of Fitzpatrick types, Skin types and Lip types\n",
    "        '''\n",
    "        messages = self.chat_prompt.fitzpatrick_skin_lip_prompt(sectionwise_text)\n",
    "        fitzpatrick_skin_lip = None\n",
    "        try:\n",
    "            fitzpatrick_skin_lip = self.llm.chat_completion(messages, **self.gen_config)\n",
    "        except Exception as e:\n",
    "            print(\"fitzpatrick_skin_lip Type Exception:\", e)\n",
    "            pass\n",
    "        return fitzpatrick_skin_lip\n",
    "\n",
    "    def instrument_attributes(self, sectionwise_text):\n",
    "        instrument = self.instrument_attr(sectionwise_text)\n",
    "        attrs_dict = {}\n",
    "        # instrument\n",
    "        if isinstance(instrument, dict):\n",
    "            attrs_dict[\"Instrument\"] = instrument.get(\"instrument\", [])\n",
    "        elif isinstance(instrument, list) or isinstance(instrument, str):\n",
    "            attrs_dict[\"Instrument\"] = instrument[:]\n",
    "        else:\n",
    "            attrs_dict[\"Instrument\"] = []\n",
    "\n",
    "        return attrs_dict\n",
    "\n",
    "    def demographics_attributes(self, sectionwise_text):\n",
    "        '''\n",
    "        Takes the sectionwise segregated text as input and returns all the demographics attributes\n",
    "        Args:\n",
    "            sectionwise_text: a dictionary with keys as section names and values as text in that section\n",
    "        Returns:\n",
    "            attrs_dict: a dictionary containing all the demographic attributes\n",
    "        '''\n",
    "        number_enrolled_completed = self.number_enrolled_completed_attr(sectionwise_text)\n",
    "        age_range = self.age_range_attr(sectionwise_text)\n",
    "        ethnicity = self.ethnicity_attr(sectionwise_text)\n",
    "        fitzpatrick_skin_lip = self.fitzpatrick_skin_lip_attr(sectionwise_text)\n",
    "        skin_type = self.skin_type_attr(sectionwise_text)\n",
    "\n",
    "        attrs_dict = {}\n",
    "\n",
    "        # Number Enrolled Completed\n",
    "        if isinstance(number_enrolled_completed, dict):\n",
    "            attrs_dict[\"Number_Enrolled\"] = number_enrolled_completed.get(\"Number_Enrolled\", None)\n",
    "            attrs_dict[\"Number_Completed\"] = number_enrolled_completed.get(\"Number_Completed\", None)\n",
    "        elif isinstance(number_enrolled_completed, str):\n",
    "            num_list = re.findall(r'\\b\\d{2,3}\\b', number_enrolled_completed, re.IGNORECASE)\n",
    "            num_list = list(map(int, num_list))\n",
    "            attrs_dict[\"Number_Enrolled\"] = max(num_list)\n",
    "            attrs_dict[\"Number_Completed\"] = min(num_list)\n",
    "        elif isinstance(number_enrolled_completed, list):\n",
    "            num_list = list(map(int, num_list))\n",
    "            attrs_dict[\"Number_Enrolled\"] = max(num_list)\n",
    "            attrs_dict[\"Number_Completed\"] = min(num_list)\n",
    "        else:\n",
    "            attrs_dict[\"Number_Enrolled\"] = None\n",
    "            attrs_dict[\"Number_Completed\"] = None\n",
    "\n",
    "        # Age Range\n",
    "        if isinstance(age_range, dict):\n",
    "            min_age = age_range.get(\"min_age\", \"\")\n",
    "            max_age = age_range.get(\"max_age\",\"\")\n",
    "            attrs_dict[\"Age_Range\"] = \"{}-{}\".format(min_age, max_age)\n",
    "        elif isinstance(age_range, list):\n",
    "            num_list = list(map(int, num_list))\n",
    "            min_age, max_age = min(num_list), max(num_list)\n",
    "            attrs_dict[\"Age_Range\"] = \"{}-{}\".format(min_age, max_age)\n",
    "        elif isinstance(age_range, str):\n",
    "            num_list = re.findall(r'\\b\\d{2}\\b', age_range, re.IGNORECASE)\n",
    "            num_list = list(map(int, num_list))\n",
    "            min_age, max_age = min(num_list), max(num_list)\n",
    "            attrs_dict[\"Age_Range\"] = \"{}-{}\".format(min_age, max_age)\n",
    "        else:\n",
    "            attrs_dict[\"Age_Range\"] = None\n",
    "\n",
    "        # Ethnicity\n",
    "        if isinstance(ethnicity, dict):\n",
    "            attrs_dict[\"Ethnicity\"] = ethnicity.get(\"Ethnicity\", [])\n",
    "        elif isinstance(ethnicity, list) or isinstance(ethnicity, str):\n",
    "            attrs_dict[\"Ethnicity\"] = ethnicity[:]\n",
    "        else:\n",
    "            attrs_dict[\"Ethnicity\"] = []\n",
    "\n",
    "\n",
    "\n",
    "        # skin_type\n",
    "        if isinstance(skin_type, dict):\n",
    "            attrs_dict[\"Skin_Type\"] = skin_type.get(\"Skin_Type\", [])\n",
    "        elif isinstance(skin_type, list) or isinstance(skin_type, str):\n",
    "            attrs_dict[\"Skin_Type\"] = skin_type[:]\n",
    "        else:\n",
    "            attrs_dict[\"Skin_Type\"] = []\n",
    "\n",
    "        # Fitzpatrick, Skin and Lip\n",
    "        attrs_dict = {**attrs_dict, \"Fitzpatrick_Type\": [], \"Skin_Tone\": [], \"Lip_Type\": []}\n",
    "        try:\n",
    "            if isinstance(fitzpatrick_skin_lip, dict):\n",
    "                fitz_list = list(set(fitzpatrick_skin_lip.get(\"Fitzpatrick_Type\", [])))\n",
    "                fitz_list = list( filter(lambda x: bool(re.search(r'[I-VI0-9]', x)),  fitz_list) )\n",
    "                attrs_dict[\"Fitzpatrick_Type\"] = [re.sub(r'\\bFitz(?:patrick)?\\b', \"Type\", fitz_type) for fitz_type in fitz_list]\n",
    "                attrs_dict[\"Skin_Tone\"] = list(set(fitzpatrick_skin_lip.get(\"Skin_Tone\", [])))\n",
    "                attrs_dict[\"Lip_Type\"] = list(set(fitzpatrick_skin_lip.get(\"Lip_Type\", [])))\n",
    "            elif isinstance(fitzpatrick_skin_lip, list):\n",
    "                fitz_list = list( filter(lambda x: bool(re.search(r'[I-VI0-9]', x)),  fitzpatrick_skin_lip) )\n",
    "                attrs_dict[\"Fitzpatrick_Type\"] = [re.sub(r'\\bFitz(?:patrick)?\\b', \"Type\", fitz_type) for fitz_type in fitz_list]\n",
    "            elif isinstance(fitzpatrick_skin_lip, str):\n",
    "                # r'\\b(?:Type\\s*[IVX]{1,3}|Fitz(?:patrick)?\\s*(?:[IVX]{1,3}|[1-6](?:/[1-6])?))\\b'\n",
    "                # fitz_list = list(set(re.findall(r\"[I-VI0-6]\", fitzpatrick_skin_lip, re.IGNORECASE)))\n",
    "                # attrs_dict[\"Fitzpatrick_Type\"] = [re.sub(r'\\bFitz(?:patrick)?\\b', \"Type\", fitz_type) for fitz_type in fitz_list]\n",
    "                if bool(re.search(r'\\[(.*)(\\])?', fitzpatrick_skin_lip)):\n",
    "                    fitz_list = list(set([fitz.strip() for fitz in re.search(r'\\[(.*)(\\])?', fitzpatrick_skin_lip).group(1).split(\",\") if fitz]))\n",
    "                    fitz_list = [re.sub(r'\"', '', fitz) for fitz in fitz_list]\n",
    "                    attrs_dict[\"Fitzpatrick_Type\"]  = list( filter(lambda x: bool(re.search(r'[I-VI0-9]', x)),  fitz_list) )\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(\"Exception during fitzpatrick_skin_lip response parsing\")\n",
    "        return attrs_dict\n",
    "\n",
    "\n",
    "\n",
    "    def table_param_y_coord(self, pdf):\n",
    "        '''\n",
    "        Finds the page number and top left y-coordinate of results tables:\n",
    "        Args:\n",
    "            pdf: input json containing tables\n",
    "        Returns:\n",
    "            lst: a list of tuples\n",
    "        '''\n",
    "        table_indices = self._get_results_tables_with_indices(pdf[\"tables\"])\n",
    "        lst = []\n",
    "        for ix in table_indices:\n",
    "            table = pdf[\"tables\"][ix]\n",
    "            lst.append((None, table[\"bounding_regions\"][0][\"page_number\"], table[\"bounding_regions\"][0][\"polygon\"][0][\"y\"]))\n",
    "        return lst\n",
    "\n",
    "\n",
    "    def regex_content_search(self, page_paragraphs, regex_pattern_1, regex_pattern_2, regex_pattern_3, y0,page_num):\n",
    "        '''\n",
    "        searches for paragraphs with given regex patterns\n",
    "        Args:\n",
    "            page_paragraphs: A list of paragraph objects (from Azure Document Intelligence output json)\n",
    "            regex_pattern_1: regex pattern to match\n",
    "            regex_pattern_2: regex pattern to match\n",
    "            regex_pattern_3: regex pattern to match\n",
    "            y0_table: top left y-coordinate of the table\n",
    "            page_num_table: page number of the table\n",
    "        Returns:\n",
    "            res_sec: A list of tuples (paragraph string, page number, top left y-coordinate of paragraph)\n",
    "        '''\n",
    "        res_sec = []\n",
    "        for para in page_paragraphs:\n",
    "            # if len(para[\"content\"].split(' ')) > 10:\n",
    "            #     continue\n",
    "            if bool((re.search(regex_pattern_1, para[\"content\"], re.IGNORECASE))):\n",
    "                if (para[\"bounding_regions\"][0][\"polygon\"][0][\"y\"] <= y0) or (para[\"bounding_regions\"][0]['page_number'] < page_num):\n",
    "                    possible_section_y = para[\"bounding_regions\"][0][\"polygon\"][0][\"y\"]\n",
    "                    if bool(re.search(regex_pattern_2, para[\"content\"], re.IGNORECASE)):\n",
    "                        res_sec.append((para[\"content\"], para[\"bounding_regions\"][0][\"page_number\"], para[\"bounding_regions\"][0][\"polygon\"][0][\"y\"]))\n",
    "                    else:\n",
    "                        bbox_para_x = [corner['x'] for corner in para[\"bounding_regions\"][0][\"polygon\"]]\n",
    "                        box_para_y = [corner['y'] for corner in para[\"bounding_regions\"][0][\"polygon\"]]\n",
    "                        for para2 in page_paragraphs:\n",
    "                            bbox_para2_x = [corner['x'] for corner in para2[\"bounding_regions\"][0][\"polygon\"]]\n",
    "                            bbox_para2_x = [corner['y'] for corner in para2[\"bounding_regions\"][0][\"polygon\"]]\n",
    "                            if bbox_para2_x == bbox_para_x and bbox_para2_y == bbox_para_y:\n",
    "                                continue\n",
    "                            current_y = para2[\"bounding_regions\"][0][\"polygon\"][0][\"y\"]\n",
    "                            if current_y == possible_section_y and bool(re.search(regex_pattern_3,para2['content'], re.IGNORECASE)):\n",
    "                                res_sec.append((para2[\"content\"], para2[\"bounding_regions\"][0][\"page_number\"], para2[\"bounding_regions\"][0][\"polygon\"][0][\"y\"]))\n",
    "        return res_sec\n",
    "\n",
    "\n",
    "    def get_section_heading_y_coords(self, paragraphs, page_num, y0_table, table_page_num):\n",
    "        '''\n",
    "        Gets a list of possible paragraphs which are section or subsection headings of results table\n",
    "        Args:\n",
    "            paragraphs: A list of paragrpah objects (from Azure Document Intelligence output json)\n",
    "            page_num: page number in which to perform the search\n",
    "            y0_table: top left y-coordinate of the table\n",
    "            page_num_table: page number of the table\n",
    "        Returns:\n",
    "            list of possible headings for the table\n",
    "        '''\n",
    "        parameters_paragraphs = [para for para in paragraphs if para[\"bounding_regions\"][0][\"page_number\"] == page_num]\n",
    "\n",
    "        sec_flag = False\n",
    "        site_flag = False\n",
    "        sub_site_flag = False\n",
    "\n",
    "        res_sec = self.regex_content_search(parameters_paragraphs, r\"\\b\\d+\\.0\\b\", r\"\\b\\d+\\.0\\b.*?RESULTS\", r\"Results\", y0_table, table_page_num)\n",
    "        if len(res_sec)>0:\n",
    "            sec_flag = True\n",
    "\n",
    "        sites = self.regex_content_search(parameters_paragraphs, r\"\\b\\d+\\.[1-9]\\b\",\n",
    "                                    r\"\\b\\d+\\.[1-9]\\b.*?(skin|dermal|ocular) evaluation\",\n",
    "                                    r\"(skin|dermal|ocular) evaluation\", y0_table, table_page_num)\n",
    "        if len(sites)>0:\n",
    "            site_flag = True\n",
    "\n",
    "        sub_sites = self.regex_content_search(parameters_paragraphs, r\"\\b\\d+\\.[1-9]\\.[1-9]\\b\",\n",
    "                                        r\"\\b\\d+\\.[1-9]\\.[1-9]\\b.*?[a-z]+\",\n",
    "                                        r\"[a-z]+\", y0_table, table_page_num)\n",
    "        if len(sub_sites)>0:\n",
    "            sub_site_flag = True\n",
    "\n",
    "        if not (sec_flag or site_flag or sub_site_flag):\n",
    "            return self.get_section_heading_y_coords(paragraphs, page_num-1, y0_table, table_page_num)\n",
    "        else:\n",
    "            if sub_site_flag:\n",
    "                return sub_sites\n",
    "            elif site_flag:\n",
    "                return sites\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "    def sort_page_numbers_and_ordinates(self, arr, parameter_tup):\n",
    "        \"\"\"\n",
    "        function to sort the list of tuples by the 2nd and 3rd value in ascending order\n",
    "        \"\"\"\n",
    "        if not arr:\n",
    "            return None\n",
    "        for i, tup in enumerate(arr):\n",
    "            if parameter_tup[1] > tup[1]:\n",
    "                continue\n",
    "            elif parameter_tup[1] <= tup[1] and parameter_tup[2] < tup[2]:\n",
    "                arr.pop(i)\n",
    "        return list(sorted(arr, key=lambda x: [x[1], x[2]]))\n",
    "\n",
    "\n",
    "    def extract_test_sites(self, pdf):\n",
    "        '''\n",
    "        Function to extract the test sites for results tables\n",
    "        '''\n",
    "        param_y_coords = self.table_param_y_coord(pdf)\n",
    "        test_sites = []\n",
    "        for i, (content, page_num, y0) in enumerate(param_y_coords):\n",
    "            tup = (content, page_num, y0)\n",
    "            arr = self.get_section_heading_y_coords(pdf[\"paragraphs\"], page_num, y0, page_num)\n",
    "            possible_list = self.sort_page_numbers_and_ordinates(arr, tup)\n",
    "            if possible_list:\n",
    "                #print(possible_list)\n",
    "                possible_test_site = possible_list[-1][0]\n",
    "                site_str = re.sub(r\"[^a-zA-Z\\s]\", \"\", possible_test_site).strip()\n",
    "                if bool(re.search(r\".*?evaluation\", site_str, re.IGNORECASE)):\n",
    "                    if bool(re.search(r\"(skin|dermal|ocular).*?evaluation.*?of\", site_str, re.IGNORECASE)):\n",
    "                        test_site = site_str.split(\" \")[-1]\n",
    "                    else:\n",
    "                        test_site = re.search(r\"(skin|dermal|ocular|eye area|lip).*?evaluation\", site_str, re.IGNORECASE).group(1) if bool(re.search(r\"(skin|dermal|ocular|eye area|lip).*?evaluation\", site_str, re.IGNORECASE)) else None\n",
    "                        #print(test_site)\n",
    "                    test_sites.append(test_site)\n",
    "                else:\n",
    "                    test_site = site_str\n",
    "                    test_sites.append(test_site)\n",
    "        return test_sites\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import perf_counter\n",
    "from promptflow.tools.common import render_jinja_template, handle_openai_error, parse_chat, to_bool, \\\n",
    "    validate_functions, process_function_call, post_process_chat_api_response, init_azure_openai_client\n",
    "\n",
    "# Avoid circular dependencies: Use import 'from promptflow._internal' instead of 'from promptflow'\n",
    "# since the code here is in promptflow namespace as well\n",
    "from promptflow._internal import enable_cache, ToolProvider, tool\n",
    "from promptflow.connections import AzureOpenAIConnection\n",
    "from promptflow.contracts.types import PromptTemplate\n",
    "\n",
    "\n",
    "class AzureOpenAI:\n",
    "    def __init__(self, connection: AzureOpenAIConnection):\n",
    "        self.connection = connection\n",
    "        self._client = init_azure_openai_client(connection)\n",
    "\n",
    "    def calculate_cache_string_for_completion(self, **kwargs,) -> str:\n",
    "        d = dict(self.connection)\n",
    "        d.pop(\"api_key\")\n",
    "        d.update({**kwargs})\n",
    "        return json.dumps(d)\n",
    "\n",
    "    @handle_openai_error()\n",
    "    @enable_cache(calculate_cache_string_for_completion)\n",
    "    def completion(\n",
    "        self,\n",
    "        prompt: PromptTemplate,\n",
    "        # for AOAI, deployment name is customized by user, not model name.\n",
    "        deployment_name: str,\n",
    "        suffix: str = None,\n",
    "        max_tokens: int = 16,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 1.0,\n",
    "        n: int = 1,\n",
    "        # stream is a hidden to the end user, it is only supposed to be set by the executor.\n",
    "        stream: bool = False,\n",
    "        logprobs: int = None,\n",
    "        echo: bool = False,\n",
    "        stop: list = None,\n",
    "        presence_penalty: float = 0,\n",
    "        frequency_penalty: float = 0,\n",
    "        best_of: int = 1,\n",
    "        logit_bias: dict = {},\n",
    "        user: str = \"\",\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        prompt = render_jinja_template(prompt, trim_blocks=True, keep_trailing_newline=True, **kwargs)\n",
    "        # TODO: remove below type conversion after client can pass json rather than string.\n",
    "        echo = to_bool(echo)\n",
    "        stream = to_bool(stream)\n",
    "        response = self._client.completions.create(\n",
    "            prompt=prompt,\n",
    "            model=deployment_name,\n",
    "            # empty string suffix should be treated as None.\n",
    "            suffix=suffix if suffix else None,\n",
    "            max_tokens=int(max_tokens),\n",
    "            temperature=float(temperature),\n",
    "            top_p=float(top_p),\n",
    "            n=int(n),\n",
    "            stream=stream,\n",
    "            # TODO: remove below type conversion after client pass json rather than string.\n",
    "            # empty string will go to else branch, but original api cannot accept empty\n",
    "            # string, must be None.\n",
    "            logprobs=int(logprobs) if logprobs else None,\n",
    "            echo=echo,\n",
    "            # fix bug \"[] is not valid under any of the given schemas-'stop'\"\n",
    "            stop=stop if stop else None,\n",
    "            presence_penalty=float(presence_penalty),\n",
    "            frequency_penalty=float(frequency_penalty),\n",
    "            best_of=int(best_of),\n",
    "            # Logit bias must be a dict if we passed it to openai api.\n",
    "            logit_bias=logit_bias if logit_bias else {},\n",
    "            user=user,\n",
    "            extra_headers={\"ms-azure-ai-promptflow-called-from\": \"aoai-tool\"})\n",
    "\n",
    "        if stream:\n",
    "            def generator():\n",
    "                for chunk in response:\n",
    "                    if chunk.choices:\n",
    "                        yield chunk.choices[0].text if hasattr(chunk.choices[0], 'text') and \\\n",
    "                               chunk.choices[0].text is not None else \"\"\n",
    "\n",
    "            # We must return the generator object, not using yield directly here.\n",
    "            # Otherwise, the function itself will become a generator, despite whether stream is True or False.\n",
    "            return generator()\n",
    "        else:\n",
    "            # get first element because prompt is single.\n",
    "            return response.choices[0].text\n",
    "\n",
    "    @handle_openai_error()\n",
    "    def chat(\n",
    "        self,\n",
    "        chat_str: str, # prompt: PromptTemplate,\n",
    "        # for AOAI, deployment name is customized by user, not model name.\n",
    "        deployment_name: str,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 1.0,\n",
    "        n: int = 1,\n",
    "        # stream is hidden to the end user, it is only supposed to be set by the executor.\n",
    "        stream: bool = False,\n",
    "        stop: list = None,\n",
    "        max_tokens: int = None,\n",
    "        presence_penalty: float = 0,\n",
    "        frequency_penalty: float = 0,\n",
    "        logit_bias: dict = {},\n",
    "        user: str = \"\",\n",
    "        # function_call can be of type str or dict.\n",
    "        function_call: object = None,\n",
    "        functions: list = None,\n",
    "        response_format: object = None,\n",
    "        seed: int = None,\n",
    "        # **kwargs,\n",
    "    ) -> [str, dict]:\n",
    "        # keep_trailing_newline=True is to keep the last \\n in the prompt to avoid converting \"user:\\t\\n\" to \"user:\".\n",
    "        # chat_str = render_jinja_template(prompt, trim_blocks=True, keep_trailing_newline=True, **kwargs)\n",
    "        messages = parse_chat(chat_str)\n",
    "        # TODO: remove below type conversion after client can pass json rather than string.\n",
    "        stream = to_bool(stream)\n",
    "        params = {\n",
    "            \"model\": deployment_name,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": float(temperature),\n",
    "            \"top_p\": float(top_p),\n",
    "            \"n\": int(n),\n",
    "            \"stream\": stream,\n",
    "            \"presence_penalty\": float(presence_penalty),\n",
    "            \"frequency_penalty\": float(frequency_penalty),\n",
    "            \"user\": user,\n",
    "            \"extra_headers\": {\"ms-azure-ai-promptflow-called-from\": \"aoai-tool\"}\n",
    "        }\n",
    "        if functions is not None:\n",
    "            validate_functions(functions)\n",
    "            params[\"functions\"] = functions\n",
    "            params[\"function_call\"] = process_function_call(function_call)\n",
    "\n",
    "        # to avoid vision model validation error for empty param values.\n",
    "        if stop:\n",
    "            params[\"stop\"] = stop\n",
    "        if max_tokens is not None and str(max_tokens).lower() != \"inf\":\n",
    "            params[\"max_tokens\"] = int(max_tokens)\n",
    "        if logit_bias:\n",
    "            params[\"logit_bias\"] = logit_bias\n",
    "        if response_format:\n",
    "            params[\"response_format\"] = response_format\n",
    "        if seed is not None:\n",
    "            params[\"seed\"] = seed\n",
    "\n",
    "        completion = self._client.chat.completions.create(**params)\n",
    "        return completion #post_process_chat_api_response(completion, stream, functions)\n",
    "\n",
    "\n",
    "# register_apis(AzureOpenAI)\n",
    "\n",
    "\n",
    "@tool\n",
    "def chat(\n",
    "    connection: AzureOpenAIConnection,\n",
    "    chat_str: str, # prompt: PromptTemplate,\n",
    "    deployment_name: str,\n",
    "    max_tokens: int\n",
    ") -> str:\n",
    "    # chat model is not available in azure openai, so need to set the environment variable.\n",
    "    gen_params = {\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,\n",
    "        \"stop\": None,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"presence_penalty\": 0.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"logit_bias\": {},\n",
    "        \"user\": \"\",\n",
    "        \"function_call\": None,\n",
    "        \"functions\": None,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"seed\": None\n",
    "    }\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    completion = AzureOpenAI(connection).chat(\n",
    "        chat_str=chat_str,\n",
    "        deployment_name=deployment_name,\n",
    "        **gen_params\n",
    "    )\n",
    "    end_time = perf_counter()\n",
    "    op_dict = {}\n",
    "    op_dict[\"response\"] = completion.choices[0].message.content\n",
    "    op_dict[\"token_stats\"] = {\n",
    "        \"prompt_tokens\": completion.usage.prompt_tokens,\n",
    "        \"completion_tokens\": completion.usage.completion_tokens,\n",
    "        \"total_tokens\": completion.usage.total_tokens\n",
    "    }\n",
    "    op_dict[\"openai_exec_time\"] = end_time - start_time\n",
    "\n",
    "    return op_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import tool\n",
    "from promptflow.connections import AzureOpenAIConnection\n",
    "from promptflow.connections import CognitiveSearchConnection\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "import requests\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import pdb\n",
    "\n",
    "\n",
    "def get_query_embedding(query, endpoint, api_key, api_version, embedding_model_deployment):\n",
    "    request_url = f\"{endpoint}/openai/deployments/{embedding_model_deployment}/embeddings?api-version={api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    request_payload = {\n",
    "        'input': query\n",
    "    }\n",
    "    embedding_response = requests.post(request_url, json=request_payload, headers=headers, timeout=None)\n",
    "    if embedding_response.status_code == 200:\n",
    "        data_values = embedding_response.json()[\"data\"]\n",
    "        embeddings_vectors = [data_value[\"embedding\"] for data_value in data_values]\n",
    "        return embeddings_vectors\n",
    "    else:\n",
    "        raise Exception(f\"failed to get embedding: {embedding_response.json()}\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def acs_retriever(\n",
    "    queries: str,\n",
    "    searchConnection: CognitiveSearchConnection,\n",
    "    indexName: str,\n",
    "    topK: int,\n",
    "    embeddingModelConnection: AzureOpenAIConnection,\n",
    "    embeddingModelName: str,\n",
    "    vectorColName: str,\n",
    "    searchType: str,\n",
    "    semantic_config_name: str,\n",
    "    query_type: str,\n",
    "    is_greeting: int\n",
    "):\n",
    "    #  searchType options: \"filter\", \"vector\", \"hybrid\", filter_vector\", \"filter_hybrid\"\n",
    "\n",
    "    if int(is_greeting):\n",
    "        return []\n",
    "\n",
    "    embeddingModelName = embeddingModelName if embeddingModelName != None else None\n",
    "\n",
    "    if not queries.endswith(\"?\"):\n",
    "        queries += \"?\"\n",
    "\n",
    "    search_client = SearchClient(\n",
    "        endpoint=searchConnection[\"api_base\"],\n",
    "        index_name=indexName,\n",
    "        credential=AzureKeyCredential(searchConnection[\"api_key\"]),\n",
    "    )\n",
    "\n",
    "    searchType = searchType.lower()\n",
    "\n",
    "    # if searchType == \"filter_vector\" or searchType == \"filter_hybrid\" or searchType == \"filter\":\n",
    "    #     filter_str = \" and \".join(f\"({key} eq '{value}')\" for key, value in filterCol.items())\n",
    "    #     filter_str = f\"({filter_str})\"\n",
    "    # else:\n",
    "    #     filter_str = None\n",
    "\n",
    "    filter_str = None\n",
    "\n",
    "    if searchType == \"vector\" or searchType == \"hybrid\" or searchType == \"filter_vector\" or searchType == \"filter_hybrid\":\n",
    "        queryEmbedding = get_query_embedding(\n",
    "            query=queries,\n",
    "            endpoint=embeddingModelConnection[\"api_base\"],\n",
    "            api_key=embeddingModelConnection[\"api_key\"],\n",
    "            api_version=embeddingModelConnection[\"api_version\"],\n",
    "            embedding_model_deployment=embeddingModelName\n",
    "        )[0]\n",
    "\n",
    "        vector_query = VectorizedQuery(vector=queryEmbedding, k_nearest_neighbors=50, fields=vectorColName, weights=0.65)\n",
    "    else:\n",
    "        vector_query=None\n",
    "\n",
    "    if searchType == \"filter\":\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries=None,\n",
    "            filter=filter_str,\n",
    "            top=topK,\n",
    "        )\n",
    "    elif searchType == \"vector\" or searchType == None:\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vector_query],\n",
    "            filter=None,\n",
    "            top=topK,\n",
    "        )\n",
    "    elif searchType == \"hybrid\":\n",
    "        results = search_client.search(\n",
    "            search_text=queries,\n",
    "            vector_queries=[vector_query],\n",
    "            query_type=query_type,\n",
    "            semantic_configuration_name=semantic_config_name,\n",
    "            filter=None,\n",
    "            top=topK,\n",
    "            select=['blob_location', 'context']\n",
    "        )\n",
    "    elif searchType == \"filter_vector\":\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries=[vector_query],\n",
    "            filter=filter_str,\n",
    "            top=topK,\n",
    "        )\n",
    "    elif searchType == \"filter_hybrid\":\n",
    "        results = search_client.search(\n",
    "            search_text=queries,\n",
    "            vector_queries=[vector_query],\n",
    "            filter=filter_str,\n",
    "            top=topK,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Please choose valid searchType from: [\"filter\", \"vector\", \"hybrid\", \"filter_vector\", \"filter_hybrid\"]')\n",
    "    output = [result for result in results]\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from promptflow.core import tool\n",
    "\n",
    "\n",
    "# def estimate_tokens(text: str) -> int:\n",
    "#   return (len(text) + 2) / 3\n",
    "\n",
    "\n",
    "@tool\n",
    "def format_retrieved_documents(docs: list, maxTokens: int) -> str:\n",
    "  formattedDocs = []\n",
    "  strResult = \"\"\n",
    "  for index, doc in enumerate(docs):\n",
    "    sourceFile = doc['blob_location'].split(\"/\")[-1].replace(r\"%20\", \" \")\n",
    "    formattedDocs.append({\n",
    "      f\"[doc{index}]\": {\n",
    "        \"title\": sourceFile,\n",
    "        \"content\": doc[\"context\"] #summarized_docs[index],\n",
    "      }\n",
    "    })\n",
    "    formattedResult = { \"retrieved_documents\": formattedDocs }\n",
    "    nextStrResult = json.dumps(formattedResult)\n",
    "    # if (estimate_tokens(nextStrResult) > maxTokens):\n",
    "    #   break\n",
    "    strResult = nextStrResult\n",
    "\n",
    "  if strResult == \"\":\n",
    "    return json.dumps({\"retrieved_documents\": []})\n",
    "\n",
    "  return strResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from promptflow.core import tool\n",
    "\n",
    "\n",
    "def format_turn(speaker: str, message: str) -> str:\n",
    "    return f\"{speaker}:\\n{message}\\n\"\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    return (len(text) + 2) / 3\n",
    "\n",
    "\n",
    "@tool\n",
    "def format_conversation(history: list, maxTokens: int) -> str:\n",
    "    result = \"\"\n",
    "    conversation_history = []\n",
    "    for history_item in history:\n",
    "        conversation_history.append({\n",
    "            \"speaker\": \"user\",\n",
    "            \"message\": history_item[\"inputs\"][\"query\"]\n",
    "        })\n",
    "        conversation_history.append({\n",
    "            \"speaker\": \"assistant\",\n",
    "            \"message\": history_item[\"outputs\"][\"response\"]\n",
    "        })\n",
    "\n",
    "    # Start using context from history, starting from most recent, until token limit is reached.\n",
    "    for turn in reversed(conversation_history):\n",
    "        turnStr = format_turn(turn[\"speaker\"], turn[\"message\"])\n",
    "        newResult = turnStr + result\n",
    "        if estimate_tokens(newResult) > maxTokens:\n",
    "            break\n",
    "        result = newResult\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import tool\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "@tool\n",
    "def add_citations(docs: list, llm_response: str) -> dict:\n",
    "\n",
    "    doc_urls = [doc['blob_location'] for doc in docs]\n",
    "\n",
    "    # Find and replace references in the message\n",
    "    formatted_message = llm_response\n",
    "    citation_list = []\n",
    "    filenames = []\n",
    "    doc_list = []\n",
    "    urls = []\n",
    "\n",
    "    # doc_dict = {element: index for index, element in enumerate(et(doc_urls[doc_index]))}\n",
    "    for match in re.finditer(r'\\[doc\\+?(\\d+)\\]', llm_response):\n",
    "        doc_index = int(match.group(1))\n",
    "\n",
    "        if doc_urls[doc_index] not in doc_list:\n",
    "            doc_list.append(doc_urls[doc_index])\n",
    "            ref_number = len(citation_list) + 1\n",
    "            # formatted_message = formatted_message.replace(f'[doc{doc_index}]', f'[{ref_number}]')\n",
    "            # formatted_message = formatted_message.replace(f'[doc{doc_index}]', '')\n",
    "            formatted_message = re.sub(rf'\\[doc\\+?{doc_index}\\]', \"\", formatted_message)\n",
    "            # Fetch URL by index, use '#' if index out of range\n",
    "            filepath = f'{doc_urls[doc_index] if doc_index < len(doc_urls) else \"#\"}'\n",
    "            filename = filepath.split(\"/\")[-1].replace(r\"%20\", \" \")\n",
    "            filenames.append(filename)\n",
    "            link = f'{filepath}'\n",
    "            # link =f'<a href=['{filename}']>{{doc_urls[doc_index] if doc_index < len(doc_urls) else \"#\"}}</a>'\n",
    "            url = f'<a href=\"{link}\">{filename}</a>'\n",
    "            urls.append(url)\n",
    "            citation_list.append(f'\\n{ref_number}.{url}')\n",
    "        else:\n",
    "            formatted_message = re.sub(rf'\\[doc\\+?{doc_index}\\]', \"\", formatted_message)\n",
    "\n",
    "    # Add citations at the end of the message\n",
    "    # formatted_message += '\\n\\n' + '\\n'.join(citation_list)\n",
    "\n",
    "    op = {\n",
    "        \"chat_output\": formatted_message,\n",
    "        \"documents\": [{\"filename\": x, \"link\": y} for x, y in zip(filenames, urls)],\n",
    "    }\n",
    "\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system:\n",
    "# You are an assistant that reformulates questions into standalone questions when necessary.\n",
    "\n",
    "# -  If the user provides a greeting (e.g., \"hi\", \"hello\", \"how are you?\", \"how's your day going?\", \"nice to meet you\"), expresses gratitude (e.g., \"thank you\", \"thanks a lot\", \"appreciate it\"), or uses acknowledgment phrases (e.g. \"okay\", \"sure\", \"got it\", \"makes sense\"), do not reformulate it. then\n",
    "#     * Ignore the chat history.\n",
    "#     * do not reformulate greetings or treat them as queries and return it as it is.\n",
    "# - If the user's follow up question is independent of the chat history, return it as-is.\n",
    "# - If the user's follow up question depends on prior context, rephrase it into a standalone question that includes the relevant details from the chat history.\n",
    "# - If the user's follow up question is combination of two or more questions, then rephrase the question such that the standalone question is representative of all the questions in the follow up question while keeping above points in check\n",
    "# - Map the following terms or any other synonymous terms (if encountered in the follow up question) to `MET Owner`:\n",
    "#     - Material Portfolio Owner\n",
    "#     - MET Leader\n",
    "#     - MET Owner\n",
    "#     **Note**: if you find the term `portfolio` at the end of the question then remove it.\n",
    "\n",
    "\n",
    "# Do not add more details than necessary to the standalone question.\n",
    "# Your response format should be as below:\n",
    "# {\"rewritten_query\": reformulated query here or the greeting, \"is_greeting\": 1, if the user provided a greeting else 0}\n",
    "\n",
    "# ## Chat history:\n",
    "# {% for item in chat_history %}\n",
    "# user:\n",
    "# {{ item.inputs.query}}\n",
    "# assistant:\n",
    "# {{ item.outputs.response }}\n",
    "# {% endfor %}\n",
    "\n",
    "# ## Follow up Question:\n",
    "# {{ query }}\n",
    "\n",
    "# ## Your Response in JSON format:"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Clinical_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
